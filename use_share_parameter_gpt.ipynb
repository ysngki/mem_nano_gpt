{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /data/yuanhang/anaconda3/envs/moe/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT, MemoryGPT\n",
    "\n",
    "from my_configuration_roberta import MemoryRobertaConfig\n",
    "from my_modeling_roberta import MemoryRobertaModel\n",
    "\n",
    "os.environ['TIKTOKEN_CACHE_DIR']=\"/data/yuanhang/tiktoken_cache_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_from = 'gpt2' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 2 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda:3' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "# device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "model = MemoryGPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# ok let's assume gpt-2 encodings by default\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load memory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 49.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MemoryRobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conversion_dense): Linear(in_features=768, out_features=9216, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "evolver_config = checkpoint['evolver_config']\n",
    "# evolver_config = MemoryRobertaConfig(vocab_size=checkpoint['model_args']['vocab_size'] + 20, num_hidden_layers=old_evolver_config.num_hidden_layers,\n",
    "#                                      num_attention_heads=old_evolver_config.num_attention_heads, \n",
    "#                                      hidden_size=old_evolver_config.hidden_size, max_position_embeddings=old_evolver_config.max_position_embeddings, \n",
    "#                                      intermediate_size=old_evolver_config.intermediate_size,\n",
    "#                                      pad_token_id=old_evolver_config.pad_token_id, gpt2_token_id_offset=old_evolver_config.gpt2_token_id_offset, \n",
    "#                                      num_memory=old_evolver_config.num_memory,\n",
    "#                                      num_target_model_layer=old_evolver_config.num_target_model_layer, no_embeddings=True)\n",
    "evolver_model = MemoryRobertaModel(evolver_config)\n",
    "state_dict = checkpoint['evolver_model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "evolver_model.load_state_dict(state_dict)\n",
    "\n",
    "evolver_model.eval()\n",
    "evolver_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 288])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4442,  1.0662, -0.2800,  ...,  0.3457, -1.1897,  0.0363],\n",
      "         [-0.1248, -0.3411,  0.0831,  ...,  0.6779, -0.6243,  1.0166],\n",
      "         [ 2.6703,  0.3512, -0.6350,  ...,  0.7345, -2.5272,  0.9817],\n",
      "         ...,\n",
      "         [-0.2002,  0.6070, -1.9469,  ..., -0.3161, -2.1979,  1.0668],\n",
      "         [ 2.4858, -0.6262,  0.1987,  ...,  0.0397,  0.1034, -0.6450],\n",
      "         [ 3.7434, -1.3990,  1.0868,  ...,  1.2074, -0.7821, -1.1913]]],\n",
      "       device='cuda:3', grad_fn=<SliceBackward0>)\n",
      "torch.Size([12, 1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# generate input memory\n",
    "\n",
    "# context = [\"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\"]\n",
    "# context = [\"We saw that Reinforce worked well. However, because we use Monte-Carlo sampling to estimate return (we use an entire episode to calculate the return), we have significant variance in policy gradient estimation.\", \n",
    "#            \"i am a handsome chinese boy. I love Japanese anime girls.\"\n",
    "#            \"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.\",\n",
    "#            ]\n",
    "# context = [\"i am a handsome chinese boy. I live in Beijing now.\"]\n",
    "context = [\"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.\"]\n",
    "\n",
    "encoded_context = []\n",
    "\n",
    "for c in context:\n",
    "    ids = encode(c)\n",
    "    ids.append(enc.eot_token)\n",
    "    encoded_context.append(torch.tensor(ids)[None, ...].to(device))\n",
    "    # encoded_context.append(torch.tensor(encode(c))[None, ...].to(device))\n",
    "\n",
    "print(encoded_context[0].shape)\n",
    "\n",
    "input_memory = None\n",
    "target_model_parameter = None\n",
    "\n",
    "for index, ec in enumerate(encoded_context):\n",
    "    output_embeds = model(idx=ec, input_parameter=target_model_parameter, output_embeds=True)\n",
    "\n",
    "    input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=input_memory)[\"memory_output\"]\n",
    "\n",
    "    # last memory -> X\n",
    "    target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "    \n",
    "    if index == 0:\n",
    "        # print(output[\"last_hidden_state\"])\n",
    "        # print(input_memory)\n",
    "        pass\n",
    "\n",
    "print(input_memory)\n",
    "print(target_model_parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34389,    12, 28286,   278,  1588,   662,    12, 35311,  3303,  4981,\n",
      "           319, 33218,  8861,   468,  1716,   262,   390,    12, 22584,    78,\n",
      "          4673, 23457,   287,   399, 19930,    13,  2102,    11, 10224, 10581,\n",
      "          3734,    12,    83,  1726,   477,   262,   220]], device='cuda:3')\n",
      "Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the ills of NLP, but they have become prohibitively prohibitive in the realm of fine-tuning small-to-moderate-to-failure (MBSB) skills. However, recent attempts to obtain a higher level of skill-level support for MBSB have proven unsuccessful. In recent years we have established various methods to integrate a simple model of the weak links in MBSB that leads to a framework-based understanding of the underlying structure of these connections. In the case of MBSB, these methods lead to an understanding of how to apply these techniques to a broad array of basic, empirical systems, including applied algebra, single- and complex-matrix systems, and computational computer systems, including such principles as, across the fields of system search, machine translation, and the assessment of existing techniques. Furthermore, the methods employed in our work are unified, providing a unified framework in which we can construct conceptual, graphical models of the concepts and methods used in different ways in different contexts. Therefore, we have used all of the methods we have used in this work to achieve the same results, with similar parameters, for all relevant parameters, directly comparable to other methods, precisely comparable to all of the parameters of that particular technique, and in all of the parameters of any other method. Although not all of the parameters of such methods have been comparable to the parameters of the existing techniques, they are all comparable to each other, using the same method, performance-equivalence-specific results allarernts and allologies applied to the specific parameters of each problem. However, all of the parameters of every other parameter of the same parameters allarernts have been successful in achieving exactly the same outcomes.\n",
      "\n",
      "Results and reasoning methods were the same as compared to the parameters of MBSB, reducing the performance of the methods using all parameters all the way, while achieving comparable results by using parameters all the way as parameters all the way, whereas applying all parameters all the way, results in exactly the same result.\n",
      "---------------\n",
      "Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the ipsis parameters and, consequently, the size of the parameter-size parameter to accommodate the observed phenomenon. As a result, the tolerances of Fine-tuning Large Pre-Tuneable Asymptotic Models ((LPI) ) have been sufficient to achieve the requisite tolerances to create a reliable set of unified methods for addressing the differences in between-subjects states of knowledge regarding the types of states of knowledge and of the corresponding states of knowledge in the states of knowledge that a student understands and recognizes. These basic principles of NLP and the Model of Syntax and Translation refer to the underlying principles of the present system, which must be implemented as a direct translation of the existing computational methods, methods and definitions to the relevant aspects. In addition, these empirical methods and methods define a broad framework for improving the understanding of the unified concepts of the various aspects of NLP applied to the unified concepts of the various areas of knowledge. Moreover, we further discover that our method of fine-tuning large-precision approaches and new approaches to the unification of the associative and non-numeric system, resulting in a system that is as efficient as the methods used, are comparable to all methods (both under comparison and not achieving all parameters). In results such as the two-parameter-sized parameters being able to achieve the same performance, parameters parameters as a whole achieved a significant reduction in the parameters parameters associated with the applied method. On the other hand, the performance of the two-parameter-size parameters became comparable to comparable methods and results were on similar parameters parameters to all the parameters of a new parameter-related problems, while the results of all three parameters achieved an equivalent representation of different parameters as parameters parameters compared to the current parameters to the respective performance of a parameter-related problem being compared to the performance of a different paramallism applied to parameters. Of course, the results of all parameters as parameters were very comparable to the parameters of all the other parameters, while each parameter performance was all the same as the parameters of all the parameters of all the parameters of all different operations were all the same.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# question = \"We saw that Reinforce worked well. \"\n",
    "question = \"i am a handsome\"\n",
    "# question = \"Do you like apple?\"\n",
    "question = \"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the \"\n",
    "\n",
    "start_ids = encode(start + question)\n",
    "x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
    "print(x)\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\n",
    "\n",
    "            result = y[0].tolist()\n",
    "\n",
    "            eot_index = len(result)\n",
    "            for ci, c in enumerate(result):\n",
    "                if c == enc.eot_token:\n",
    "                    eot_index = ci\n",
    "                    break\n",
    "            \n",
    "            print(decode(result[:eot_index]))\n",
    "            print('---------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
