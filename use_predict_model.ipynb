{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import MemoryGPT as GPT\n",
    "from peft import prepare_model_for_int8_training\n",
    "\n",
    "from my_configuration_roberta import MemoryRobertaConfig\n",
    "from my_modeling_roberta import MemoryRobertaModel\n",
    "\n",
    "os.environ['TIKTOKEN_CACHE_DIR']=\"/data/yuanhang/tiktoken_cache_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 2 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "\n",
    "device = 'cuda:0' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "# device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "[27, 91, 437, 1659, 5239, 91, 29]\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = 'gpt2'\n",
    "\n",
    "# load pretrained model\n",
    "if \"gpt\" in pretrained_model:\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {pretrained_model}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    model = GPT.from_pretrained(pretrained_model, dict(dropout=0.0))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # backbone forzen\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    \n",
    "    pretrained_model_config = model.config\n",
    "else:\n",
    "    raise Exception(f\"Unrecognized pretrained model {pretrained_model}\")\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# ok let's assume gpt-2 encodings by default\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "# encode = lambda s: enc.encode_ordinary(s)\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "    \n",
    "print(enc.encode_ordinary(\"<|endoftext|>\"))\n",
    "print(enc.encode(\"<|endoftext|>\",  allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load memory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 49.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MemoryRobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conversion_dense): Linear(in_features=768, out_features=9216, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_method = \"prompt\"\n",
    "# peft_method = \"lora\"\n",
    "\n",
    "ckpt_path = os.path.join(out_dir, 'predict_1st_kl_1_seg.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "evolver_config = checkpoint['evolver_config']\n",
    "evolver_model = MemoryRobertaModel(evolver_config)\n",
    "state_dict = checkpoint['evolver_model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "evolver_model.load_state_dict(state_dict)\n",
    "\n",
    "evolver_model.eval()\n",
    "evolver_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"iter_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(question, input_parameter=None, peft=\"prompt\"):\n",
    "    start_ids = encode(question)\n",
    "    x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...][:, -512:]\n",
    "    input_length = x.shape[1]\n",
    "    print(x)\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(1):\n",
    "                # y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\n",
    "                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=input_parameter, peft=peft)\n",
    "\n",
    "                result = y[0].tolist()[input_length:]\n",
    "\n",
    "                eot_index = len(result)\n",
    "                for ci, c in enumerate(result):\n",
    "                    if c == enc.eot_token:\n",
    "                        eot_index = ci\n",
    "                        break\n",
    "                \n",
    "                print(decode(result[:eot_index]))\n",
    "                print('---------------')\n",
    "            print('===============================================================')\n",
    "\n",
    "\n",
    "def generate_parameter(context_list=None, context_id_list=None):\n",
    "    if context_list is not None:\n",
    "        encoded_context = []\n",
    "\n",
    "        for c in context_list:\n",
    "            ids = encode(c)\n",
    "            # ids.append(enc.eot_token)\n",
    "            encoded_context.append(torch.tensor(ids)[None, ...].to(device))\n",
    "    \n",
    "    if context_id_list is not None:\n",
    "        encoded_context = context_id_list\n",
    "        \n",
    "    input_memory = None\n",
    "    target_model_parameter = None\n",
    "\n",
    "    for index, ec in enumerate(encoded_context):\n",
    "        output_embeds = model(idx=ec, output_embeds=True)\n",
    "\n",
    "        input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=input_memory)[\"memory_output\"]\n",
    "\n",
    "    # last memory -> X\n",
    "    target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "\n",
    "    # print(input_memory)\n",
    "    # print(target_model_parameter.shape)\n",
    "    \n",
    "    return target_model_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 383,  717, 1438,  286,  262, 1459, 1294, 1893,  318,  366]],\n",
      "       device='cuda:0')\n",
      "Donald Trump.\" Trump is the oldest of the Republican candidates, and the first and probably only person president with an actual corporate name.\n",
      "\n",
      "And yet, the size of his campaign is astonishing, and he's ready to move the American people in a new direction.\n",
      "\n",
      "Trump's campaign, speaking to the National Enquirer, said there's a \"huge resistance\" in Washington to the idea of him and his son, Donald Jr., joining the class of 2016.\n",
      "\n",
      "\"The idea of a Trump campaign in which there is very little consensus in Washington is unconscionable, morally wrong,\" said Doug Stafford, a political scientist at John Jay College of Criminal Justice.\n",
      "\n",
      "\"The reality is that if Donald Trump is elected, there will be a revolt against his idea of an American exceptionalism,\" said Stafford, adding that the Republican nominee will also make it clear he opposes the Constitution – a theme that has become increasingly central to the Republican platform.\n",
      "\n",
      "Trump's campaign is already ramping up its campaign against the Constitution, and currently wants federal judges to strike down portions of the order they claim is unconstitutional. (The Supreme Court won't rule on the order next week, however.)\n",
      "\n",
      "But even if they won't rule on the new order, Republicans will accuse the administration of standing behind Trump until it gets his finger on the ball to enact a vision that begins with a constitutional amendment and ends with a global treaty.\n",
      "\n",
      "\"He is the man,\" said Roger Stone, a dissenter on the Trump campaign. \"He knows how to win.\"\n",
      "\n",
      "\"He's the only one in Washington who understands that the Constitution is not the right word on a constitutional issue,\" Stone added. \"But the people must embrace a promise to repeal his executive order that says we are America first and America second.\"\n",
      "---------------\n",
      "===============================================================\n",
      "tensor([[ 383,  717, 1438,  286,  262, 1459, 1294, 1893,  318,  366]],\n",
      "       device='cuda:0')\n",
      "Nixon\n",
      "\n",
      "President\"\n",
      "\n",
      "Name of current US president\n",
      "\n",
      "Name of current US president\n",
      "\n",
      "When Barack Obama was President of the United States\n",
      "\n",
      "Name of current US president\n",
      "\n",
      "Name of current US president\n",
      "\n",
      "The Republican party's nominee for president, presidential candidate and Republican National Committee Chairman, John McCain, is the son of a Republican congressman from Arizona. John McCain was born Oct. 4, 1961, in Peoria, Illinois.\n",
      "\n",
      "\n",
      "HALTON SLC\n",
      "---------------\n",
      "===============================================================\n",
      "tensor([[19585, 21010,    11,  4642,   287,  1446,  5250,   261,    11,  9589,\n",
      "            11,   319,  3389,  1160,    11, 22458,    11,   550,   257, 12949,\n",
      "         40678,   287,   257,  3504,    12,  4871,  1641,    13,   679,  9141,\n",
      "           262,  2059,   286, 19603,    11,   220,   383,   717,  1438,   286,\n",
      "           262,  1459,  1294,  1893,   318,   366]], device='cuda:0')\n",
      "Biden.\" He enrolled at the University of Washington with his family when he was a sophomore. In 1973, he joined the Navy. A year later, he became a Marine Corps Marine and served in Vietnam. He took command of the 18th Marine Division south of Baghdad, Iraq. During his time in Iraq, he served in a variety of roles. Among the toughest roles he was involved in was serving as a liaison officer to Baghdad. He assisted the Iraqi forces in building up the logistics infrastructure needed to win the battle for the city. He helped get the ISIS control of the area and, later, Iraq's oil resources. As a direct result of the Iraq War, he was awarded the Congressional Gold Medal for valor, the first civilian award he had received for his service. In 2003, Joe Biden married Livia F. Ford. They have two children. He has three grandchildren.\n",
      "\n",
      "Eagle Eye, the Obama Web site, appears to be the largest internet news site in the world. The site includes links to hundreds of articles on Hillary Clinton and the affair of former president Bill Clinton. The site is used by visitors to the site.\n",
      "\n",
      "Eagle Eye\n",
      "\n",
      "Eagle Eye News\n",
      "\n",
      "Eagle Eye, a formerly affiliated site, is a subsidiary of the Washington Post of the Society of Military Editors, Inc.\n",
      "\n",
      "Eagle Eye's editors, the Society of Military Editors, Inc., publish articles in the Washington Post, Newsweek and \"The New York Times.\"\n",
      "\n",
      "Eagle Eye's editors, the Society of Military Editors, Inc., publish articles in the Washington Post, Newsweek and \"The New York Times.\" Eagle Eye is a subsidiary company of the Washington Post of the Societe Generale d'économique Española in Lausanne, Switzerland. It is a subsidiary of the Washington Post of the Societe Generale d'économique Española, and its website www.elawilenteanews.com is under the Web name Eagle Eye News.\n",
      "\n",
      "Eagle Eye's editors, the Society of Military Editors, Inc., publish articles in the Washington Post, Newsweek and \"The New York Times. Eagle Eye's editors, the Society of Military Editors, Inc., publish articles in the Washington Post, Newsweek and \"The New York Times.\" Eagle Eye is a subsidiary company of the Washington Post of the Societe Generale d'économique Española in La\n",
      "---------------\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "question = \"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored\"\n",
    "question = ' The first name of the current US president is \"'\n",
    "\n",
    "generate_sentence(question)\n",
    "\n",
    "context = [\"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, \"]\n",
    "# context = [\n",
    "#             \"\"\"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored in history and political science, graduating in 1965. Afterward, he earned his law degree from Syracuse University College of Law in 1968.\\nBiden's early political career began in 1970 when he was elected to the New Castle County Council in Delaware. In 1972, tragedy struck when his wife Neilia and 1-year-old daughter Naomi were killed in a car accident, and his two sons, Beau and Hunter, were injured. Despite this devastating loss, Biden chose to honor his commitment and was sworn in as a senator by his sons' hospital bedsides.\\nHe went on to serve as the United States Senator from Delaware for six terms, from 1973 to 2009. \"\"\",\n",
    "#             \"\"\"During his time in the Senate, Biden was involved in various committees and was particularly known for his expertise in foreign affairs, serving as the chairman of the Senate Foreign Relations Committee on multiple occasions.\\nIn 2008, Joe Biden was selected as the running mate for Barack Obama, who went on to win the presidential election. As Vice President, Biden played an integral role in the Obama administration, helping to shape policies and handling issues such as economic recovery, foreign relations, and the implementation of the Affordable Care Act (ACA), commonly known as Obamacare.\\nAfter completing two terms as Vice President, Joe Biden decided to run for the presidency in 2020. He secured the Democratic nomination and faced the incumbent President Donald Trump in the general election. Biden campaigned on a platform of unity, promising to heal the divisions in the country and tackle pressing issues, including the COVID-19 pandemic, climate change, racial justice, and economic inequality.\\nIn the November 2020 election, Biden emerged victorious, and on January 20, 2021, he was inaugurated as the 46th President of the United States. At the age of 78, Biden became the oldest person to assume the presidency in American history.\\nAs President, Joe Biden has worked to implement his agenda, focusing on various initiatives, such as infrastructure investment, climate action, immigration reform, and expanding access to healthcare. He has emphasized the importance of diplomacy in international relations and has sought to rebuild alliances with global partners.\\nThroughout his long career in public service, Joe Biden has been recognized for his commitment to bipartisanship, empathy, and his dedication to working-class issues. He continues to navigate the challenges facing the nation, striving to bring the country together and create positive change for all Americans.\"\"\"\n",
    "#         ]\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "generate_sentence(question, input_parameter, peft_method)\n",
    "\n",
    "context_question = \"\"\n",
    "for c in context:\n",
    "    context_question += c\n",
    "context_question += question\n",
    "generate_sentence(context_question)\n",
    "\n",
    "\n",
    "# context = [\"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\"]\n",
    "# context = [\"We saw that Reinforce worked well. However, because we use Monte-Carlo sampling to estimate return (we use an entire episode to calculate the return), we have significant variance in policy gradient estimation.\", \n",
    "#            \"i am a handsome chinese boy. I love Japanese anime girls.\"\n",
    "#            \"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.\",\n",
    "#            ]\n",
    "# context = [\"i am a handsome chinese boy.\"]\n",
    "context = [\"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.\"]\n",
    "context = [\"\"\"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored in history and political science, graduating in 1965. Afterward, he earned his law degree from Syracuse University College of Law in 1968.\\nBiden's early political career began in 1970 when he was elected to the New Castle County Council in Delaware. In 1972, tragedy struck when his wife Neilia and 1-year-old daughter Naomi were killed in a car accident, and his two sons, Beau and Hunter, were injured. Despite this devastating loss, Biden chose to honor his commitment and was sworn in as a senator by his sons' hospital bedsides.\\nHe went on to serve as the United States Senator from Delaware for six terms, from 1973 to 2009. During his time in the Senate, Biden was involved in various committees and was particularly known for his expertise in foreign affairs, serving as the chairman of the Senate Foreign Relations Committee on multiple occasions.\\nIn 2008, Joe Biden was selected as the running mate for Barack Obama, who went on to win the presidential election. As Vice President, Biden played an integral role in the Obama administration, helping to shape policies and handling issues such as economic recovery, foreign relations, and the implementation of the Affordable Care Act (ACA), commonly known as Obamacare.\\nAfter completing two terms as Vice President, Joe Biden decided to run for the presidency in 2020. He secured the Democratic nomination and faced the incumbent President Donald Trump in the general election. Biden campaigned on a platform of unity, promising to heal the divisions in the country and tackle pressing issues, including the COVID-19 pandemic, climate change, racial justice, and economic inequality.\\nIn the November 2020 election, Biden emerged victorious, and on January 20, 2021, he was inaugurated as the 46th President of the United States. At the age of 78, Biden became the oldest person to assume the presidency in American history.\\nAs President, Joe Biden has worked to implement his agenda, focusing on various initiatives, such as infrastructure investment, climate action, immigration reform, and expanding access to healthcare. He has emphasized the importance of diplomacy in international relations and has sought to rebuild alliances with global partners.\\nThroughout his long career in public service, Joe Biden has been recognized for his commitment to bipartisanship, empathy, and his dedication to working-class issues. He continues to navigate the challenges facing the nation, striving to bring the country together and create positive change for all Americans.\"\"\"]\n",
    "context = ['The name of the current US president is \"Joe Biden\".']\n",
    "context = ['Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. ']\n",
    "\n",
    "question = \"However, conventional approaches fine-tune all the \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7923, device='cuda:0')\n",
      "tensor(3.5367, device='cuda:0')\n",
      "tensor(3.8945, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context = [\"\"\"I've been reading a lot of articles on nanomedicine and crispr, and it's all really interesting to me. How much about ourselves would we be able to change once we finally crack the code of our own bodies? Would we be able to change our skeletal structure? Eye color? Could we alter our immune system to be more effective against viruses?\"\"\"]\n",
    "context = [\"\"\"When he grew up to an age fit for going to school, he was put under the care of the rev. Mr. Naish at Ambrosbury. He afterwards removed to a school at Salisbury, taught by the rev. Mr. Taylor, thence to the Charter-house, where he was under the tuition of the learned Dr. Ellis, and where he contracted an intimacy with Mr. Steel, afterwards Sir Richard, which continued as long as Mr. Addison lived.\"\"\"]\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "\n",
    "x = \"\"\"I know currently none of this is even remotely possible, but it feels like we're dusting off the stepping stones to some pretty promising prospects, and I'm curious to see how fast things will go once we get the ball rolling. Or if, y'know, we all die before even a modicum of useful information is found.\"\"\"\n",
    "x = \"\"\"He was not above fifteen years old when he was entered of Queen's College, Oxford, in which his father had been placed: where he applied himself so closely to the study of classical learning, that in a very short time he became master of a very elegant Latin stile, even before he arrived at that age when ordinary scholars begin to write good English.\"\"\"\n",
    "x = encode(x)\n",
    "x = torch.tensor(x, dtype=torch.long, device=device)[None, ...]\n",
    "y = x[:, 1:]\n",
    "x = x[:, :-1]\n",
    "\n",
    "_, loss = model(x, y)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "encoded_context = torch.tensor(encode(context[0]), dtype=torch.long, device=device)[None, ...]\n",
    "padding_y = torch.full_like(encoded_context, fill_value=-1, dtype=torch.long, device=device)\n",
    "\n",
    "context_x = torch.concatenate((encoded_context, x), dim=1)\n",
    "context_y = torch.concatenate((padding_y, y), dim=1)\n",
    "_, loss = model(context_x, context_y)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "_, loss = model(x, y, input_parameter=input_parameter, peft=peft_method)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3066539608\n",
      "tensor(4.2906, device='cuda:0')\n",
      "tensor(3.9497, device='cuda:0')\n",
      "tensor(3.8115, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = \"pg19\"\n",
    "# dataset = \"openwebtext\"\n",
    "\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "# val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "print(len(train_data))\n",
    "\n",
    "####-----------------------------------------------------\n",
    "context_start = 42182322\n",
    "input_start = context_start + 512\n",
    "input_end = input_start + 512\n",
    "\n",
    "####-----------------------------------------------------\n",
    "# context = [decode(train_data[context_start:input_start-256]), decode(train_data[input_start-256:input_start])]\n",
    "context = [decode(train_data[context_start:input_start])]\n",
    "context_str = \"\"\n",
    "for s in context:\n",
    "    context_str += s\n",
    "\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "x_str = decode(train_data[input_start:input_end])\n",
    "y_str = decode(train_data[input_start + 1:input_end + 1])\n",
    "\n",
    "x = torch.from_numpy(train_data[input_start:input_end].astype(np.int64)).unsqueeze(0).to(device)\n",
    "y = torch.from_numpy(train_data[input_start + 1:input_end + 1].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "# for s in [context_str, x_str, y_str]:\n",
    "#     print(s)\n",
    "#     print(\"-\"*100)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "_, loss = model(x, y)\n",
    "print(loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "encoded_context = torch.tensor(encode(context_str), dtype=torch.long, device=device)[None, ...]\n",
    "padding_y = torch.full_like(encoded_context, fill_value=-1, dtype=torch.long, device=device)\n",
    "\n",
    "context_x = torch.concatenate((encoded_context, x), dim=1)[:, -1024:]\n",
    "context_y = torch.concatenate((padding_y, y), dim=1)[:, -1024:]\n",
    "_, loss = model(context_x, context_y)\n",
    "print(loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "_, loss = model(x, y, input_parameter=input_parameter, peft=peft_method)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the 6th of May\n",
      "1672; and being not thought likely to live, was baptized on the same\n",
      "day, as appears from the church register. When he grew up to an age fit\n",
      "for going to school, he was put under the care of the rev. Mr. Naish at\n",
      "Ambrosbury. He afterwards removed to a school at Salisbury, taught by\n",
      "the rev. Mr. Taylor, thence to the Charter-house, where he was under the\n",
      "tuition of the learned Dr. Ellis, and where he contracted an intimacy\n",
      "with Mr. Steel, afterwards Sir Richard, which continued as long as Mr.\n",
      "Addison lived. He was not above fifteen years old when he was entered of\n",
      "Queen's College, Oxford, in which his father had been placed: where he\n",
      "applied himself so closely to the study of classical learning, that in\n",
      "a very short time he became master of a very elegant Latin stile, even\n",
      "before he arrived at that age when ordinary scholars begin to write good\n",
      "English.\n",
      "\n",
      "In the year 1687 a copy of his verses in that tongue fell into the hands\n",
      "of Dr. Lancaster dean of Magdalen College, who was so pleased with them,\n",
      "that he immediately procured their author's election into that house\n",
      "[1]; where he took the degrees of bachelor, and matter of arts. In the\n",
      "course of a few years his Latin poetry was justly admired at both the\n",
      "universities, and procured him great reputation there, before his name\n",
      "was so much as known in London. When he was in the 22d year of his\n",
      "age, he published a copy of verses addressed to Mr. Dryden, which soon\n",
      "procured him the notice of some of the poetical judges in that age. The\n",
      "verses are not without their elegance, but if they are much removed\n",
      "above common rhimes, they fall infinitely short of the character Mr.\n",
      "Addison's friends bestowed upon them. Some little space intervening, he\n",
      "sent into the world a translation of the 4th Georgic of Virgil, of which\n",
      "we need not say any more, than that it was commended by Mr. Dryden. He\n",
      "wrote also that discourse on the Georgics, prefixed to them by way of\n",
      "preface in Mr. Dryden's translation, and chose to withhold his name from\n",
      "that judicious composition, because it contained an untried strain of\n",
      "criticism, which bore hard\n",
      "----------------------------------------------------------------------------------------------------\n",
      " upon the old professors of that art, and\n",
      "therefore was not so fit for a young man to take upon himself; and Mr.\n",
      "Dryden, who was above the meanness of fathering any one's work, owns\n",
      "the Essay on the Georgics to have come from a friend, whose name is not\n",
      "mentioned, because he desired to have it concealed.\n",
      "\n",
      "The next year Mr. Addison wrote several poems of different kinds;\n",
      "amongst the rest, one addressed to Henry Sacheverel, who became\n",
      "afterwards so exceedingly famous. The following year he wrote a poem to\n",
      "King William on one of his Campaigns, addressed to the Lord Keeper (Sir\n",
      "John Somers.) That excellent statesman received this mark of a young\n",
      "author's attachment with great humanity, admitted Mr. Addison into the\n",
      "number of his friends, and gave him on all occasions distinguishing\n",
      "proofs of a sincere esteem [2]. While he was at the university, he had\n",
      "been pressingly sollicited to enter into holy orders, which he seemed\n",
      "once resolved on, probably in obedience to his father's authority; but\n",
      "being conscious of the importance of the undertaking, and deterred by\n",
      "his extreme modesty, he relinquished, says Mr. Tickell, all views that\n",
      "way; but Sir Richard Steel in his letter to Mr. Congreve prefixed to\n",
      "the Drummer, who had a quarrel with Tickell, on account of an injurious\n",
      "treatment of him, says, that those were not the reasons which made Mr.\n",
      "Addison turn his thoughts to the civil world, 'and as you were the\n",
      "inducement (says he) of his becoming acquainted with my lord Hallifax,\n",
      "I doubt not but you remember the warm instances that noble lord made\n",
      "to the head of the college, not to insist on Mr. Addison's going into\n",
      "orders; his arguments were founded on the general pravity and\n",
      "corruption of men of business, who wanted liberal education; and I\n",
      "remember, as if I had read the letter yesterday, that my lord ended with\n",
      "a compliment, that however he might be represented as no friend to the\n",
      "church, he would never do it any other injury than by keeping\n",
      "Mr. Addison out of it.'\n",
      "\n",
      "Mr. Addison having discovered an inclination to travel, the\n",
      "abovementioned patron, out of zeal, as well to his country, as our\n",
      "author, procured him\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(context_str)\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(x_str)\n",
    "print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
