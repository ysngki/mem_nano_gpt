{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT, MemoryGPT\n",
    "\n",
    "from my_configuration_roberta import MemoryRobertaConfig\n",
    "from my_modeling_roberta import MemoryRobertaModel\n",
    "\n",
    "os.environ['TIKTOKEN_CACHE_DIR']=\"/data/yuanhang/tiktoken_cache_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_from = 'gpt2' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 2 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda:3' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = MemoryGPT.from_pretrained(init_from, dict(dropout=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No meta.pkl found, assuming GPT-2 encodings...\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 88.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "evolver_config = MemoryRobertaConfig(vocab_size=checkpoint['model_args']['vocab_size'] + 20, num_hidden_layers=6,\n",
    "                                     num_attention_heads=12, hidden_size=768, max_position_embeddings=512, intermediate_size=3072,\n",
    "                                     pad_token_id=0, gpt2_token_id_offset=20, num_memory=10,\n",
    "                                     num_target_model_layer=12)\n",
    "evolver_model = MemoryRobertaModel(evolver_config)\n",
    "state_dict = checkpoint['evolver_model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "evolver_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50277, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conversion_dense): Linear(in_features=768, out_features=9216, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evolver_model.eval()\n",
    "evolver_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1861,  1.1499,  0.1170,  ..., -0.6563, -0.3641, -0.5765],\n",
      "         [ 1.1784,  1.1462,  0.1218,  ..., -0.6369, -0.3493, -0.5873],\n",
      "         [ 1.2003,  1.1558,  0.1102,  ..., -0.6811, -0.3793, -0.5673],\n",
      "         ...,\n",
      "         [ 1.1719,  1.1428,  0.1269,  ..., -0.6271, -0.3427, -0.5891],\n",
      "         [ 1.1809,  1.1463,  0.1191,  ..., -0.6475, -0.3576, -0.5813],\n",
      "         [ 1.1782,  1.1464,  0.1197,  ..., -0.6444, -0.3568, -0.5823]]],\n",
      "       device='cuda:3', grad_fn=<SliceBackward0>)\n",
      "torch.Size([12, 1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# generate input memory\n",
    "\n",
    "context = [\"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\"]\n",
    "# context = [\"i am a handsome chinese boy. I live in Beijing now.\"]\n",
    "# context = [\"i am a handsome chinese boy.\"]\n",
    "\n",
    "encoded_context = []\n",
    "\n",
    "for c in context:\n",
    "    encoded_context.append(torch.tensor(encode(c))[None, ...].to(device) + 20)\n",
    "\n",
    "input_memory = None\n",
    "for index, ec in enumerate(encoded_context):\n",
    "    output = evolver_model(input_ids=ec, input_memory=input_memory)\n",
    "    input_memory = output[\"memory_output\"]\n",
    "    \n",
    "    if index == 0:\n",
    "        # print(output[\"last_hidden_state\"])\n",
    "        # print(input_memory)\n",
    "        pass\n",
    "\n",
    "print(input_memory)\n",
    "\n",
    "target_model_memory = evolver_model(input_memory=input_memory, produce_memory_flag=True)\n",
    "print(target_model_memory.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 72, 716, 257, 220]], device='cuda:3')\n",
      "i am a vernacular the world over. . The major media doesn’t do the business of the world. The Internet is a fucking internet. I don’t care how many people I know have commented on this. My information is in the public domain. I don’t care. It’s my life. I don’t care. I just need to be polite and helpful. I do not care about personal info, I’m absolutely fine with that. When I’m just beginning, I want to get a nudge. I want to let people know how they’ll be able to decide what they want and what they want to see of me.\n",
      "---------------\n",
      "i am a ursa. As a saur on my own I am not raved about, but I am pleased with how well it is going. I got a 9.5 on a custom motorcycle and the new one is a 9.9. The bike is good so far, and the engine is quiet, but I am not a fan of the turbocharger on the new bike. The throttle is pretty much the same as the old one, with all fours, but they are a bit out of balance. The throttle is good so far, and the fuel gauge is pretty good, so far. The bike is very quiet, but I am a bit worried. I have to be at the shop for a break now before it gets to the 10:00 mark. The bike is brand new, and I am getting pretty beat up.\n",
      "\n",
      "Rated 5 out of 5 by P. from I am glad I bought my bike. It is a good bike, easy to set up, and I was looking to get the most efficient ride on the highway. It is much quieter than the old one, and the new one has a raised front section that I have never seen before. When I first rode it I thought it would be a 5.6, but it is for sale at $395, so I will wait and see. But then I started to notice its not good, where the revs are unbelievable, where the throttle is super low, and the pokes are a bit off. Now that I am done with the new one, it is the best bike I have ever bought.\n",
      "\n",
      "Rated 5 out of 5 by Yol from Love it This is the best bike I have ever ridden. I am a very satisfied customer. I found my new bike at a dealership and decided to buy it to see what it can do. I am really looking forward to the ability to ride a new bike in the future.\n",
      "\n",
      "Rated 5 out of 5 by Fela from A great bike My new bike arrived in two days. It is a very comfortable bike. My problem was that the brake pedal stops working after a few days, therefore, I ordered a new brake pedal. I got the bike from the dealership and it looks great. It is not very quiet and I am happy my car switched to new. I also have a new front fork for the new bike, it is a bit more quiet when I am not riding on the highway.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "question = \"i am a \"\n",
    "\n",
    "start_ids = encode(start + question)\n",
    "x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
    "print(x)\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_memory=target_model_memory)\n",
    "\n",
    "            result = y[0].tolist()\n",
    "\n",
    "            eot_index = len(result)\n",
    "            for ci, c in enumerate(result):\n",
    "                if c == enc.eot_token:\n",
    "                    eot_index = ci\n",
    "                    break\n",
    "            \n",
    "            print(decode(result[:eot_index]))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 716, 257, 22665, 442, 3762, 2933, 13, 314, 2107]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i am a<|endoftext|> chinese boy. I live'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"i am a handsome chinese boy. I live\"\n",
    "\n",
    "start_ids = encode(start + question)\n",
    "\n",
    "print(start_ids)\n",
    "\n",
    "my_ids = [72, 716, 257, 22665, 442, 3762, 2933, 13, 314, 2107]\n",
    "\n",
    "decode(my_ids)\n",
    "\n",
    "my_ids = [72, 716, 257, enc.eot_token, 442, 3762, 2933, 13, 314, 2107]\n",
    "\n",
    "decode(my_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
