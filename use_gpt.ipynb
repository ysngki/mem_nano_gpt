{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT, MemoryGPT\n",
    "\n",
    "from my_configuration_roberta import MemoryRobertaConfig\n",
    "from my_modeling_roberta import MemoryRobertaModel\n",
    "\n",
    "os.environ['TIKTOKEN_CACHE_DIR']=\"/data/yuanhang/tiktoken_cache_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_from = 'gpt2' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 2 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda:3' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = MemoryGPT.from_pretrained(init_from, dict(dropout=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No meta.pkl found, assuming GPT-2 encodings...\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 88.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'second_pure_mem_one_seg_ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "# evolver_config = MemoryRobertaConfig(vocab_size=checkpoint['model_args']['vocab_size'] + 20, num_hidden_layers=6,\n",
    "#                                      num_attention_heads=12, hidden_size=768, max_position_embeddings=512, intermediate_size=3072,\n",
    "#                                      pad_token_id=0, gpt2_token_id_offset=20, num_memory=10,\n",
    "#                                      num_target_model_layer=12)\n",
    "old_evolver_config = checkpoint['evolver_config']\n",
    "evolver_config = MemoryRobertaConfig(vocab_size=checkpoint['model_args']['vocab_size'] + 20, num_hidden_layers=old_evolver_config.num_hidden_layers,\n",
    "                                     num_attention_heads=old_evolver_config.num_attention_heads, \n",
    "                                     hidden_size=old_evolver_config.hidden_size, max_position_embeddings=old_evolver_config.max_position_embeddings, \n",
    "                                     intermediate_size=old_evolver_config.intermediate_size,\n",
    "                                     pad_token_id=old_evolver_config.pad_token_id, gpt2_token_id_offset=old_evolver_config.gpt2_token_id_offset, \n",
    "                                     num_memory=old_evolver_config.num_memory,\n",
    "                                     num_target_model_layer=old_evolver_config.num_target_model_layer, no_embeddings=False)\n",
    "evolver_model = MemoryRobertaModel(evolver_config)\n",
    "state_dict = checkpoint['evolver_model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "evolver_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryRobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50277, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conversion_dense): Linear(in_features=768, out_features=9216, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evolver_model.eval()\n",
    "evolver_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 288])\n",
      "tensor([[[-0.0690,  0.3496,  0.1826,  ..., -0.3199, -0.5849,  0.2867],\n",
      "         [ 0.3603,  0.0108, -0.2806,  ..., -0.2341, -0.2594,  0.0499],\n",
      "         [-0.0931,  0.0535, -0.1141,  ...,  0.2645,  0.0402, -0.2185],\n",
      "         ...,\n",
      "         [ 0.0330, -0.0155,  0.0143,  ..., -0.2151, -0.0232, -0.1247],\n",
      "         [-0.4123,  0.2260,  0.1207,  ...,  0.0245,  0.2943,  0.1861],\n",
      "         [-0.6130, -0.1130,  0.3414,  ..., -0.6120,  0.4928,  0.2089]]],\n",
      "       device='cuda:3', grad_fn=<SliceBackward0>)\n",
      "torch.Size([12, 1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# generate input memory\n",
    "\n",
    "# context = [\"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\"]\n",
    "context = [\"We saw that Reinforce worked well. However, because we use Monte-Carlo sampling to estimate return (we use an entire episode to calculate the return), we have significant variance in policy gradient estimation.\", \n",
    "           \"i am a handsome chinese boy. I love Japanese anime girls.\"\n",
    "           \"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.\",\n",
    "           ]\n",
    "# context = [\"i am a handsome chinese boy.\"]\n",
    "context = [\"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.\"]\n",
    "\n",
    "\n",
    "encoded_context = []\n",
    "\n",
    "for c in context:\n",
    "    ids = encode(c)\n",
    "    ids.append(enc.eot_token)\n",
    "    encoded_context.append(torch.tensor(ids)[None, ...].to(device) + evolver_config.gpt2_token_id_offset)\n",
    "    # encoded_context.append(torch.tensor(encode(c))[None, ...].to(device))\n",
    "\n",
    "print(encoded_context[0].shape)\n",
    "\n",
    "input_memory = None\n",
    "for index, ec in enumerate(encoded_context):\n",
    "    output = evolver_model(input_ids=ec, input_memory=input_memory)\n",
    "    input_memory = output[\"memory_output\"]\n",
    "    \n",
    "    if index == 0:\n",
    "        # print(output[\"last_hidden_state\"])\n",
    "        # print(input_memory)\n",
    "        pass\n",
    "\n",
    "print(input_memory)\n",
    "\n",
    "target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "print(target_model_parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34389,    12, 28286,   278,  1588,   662,    12, 35311,  3303,  4981,\n",
      "           319, 33218,  8861,   468,  1716,   262,   390,    12, 22584,    78,\n",
      "          4673, 23457,   287,   399, 19930,    13,  2102,    11, 10224, 10581,\n",
      "          3734,    12,    83,  1726,   477,   262,   220]], device='cuda:3')\n",
      "Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the Rs parameters, which becomes prohibitively large and the model size grows as tasks. Recent work is proposed by task growth and the number of parameter transfer of fine-tuning parameters that are small (e.m) examples to attain a small (effective) performance. While effective ingredients, the critical ingredients for the connections among well-tuned and the various methods are poorly understood. In this paper, we break down the design of state-of-art parameter transfer theory and present a unified framework that establishes a unified framework between them. Specifically, we reframe them as modifications to pre-trained hidden states in pre-trained models, and define a set of design dimensions along which different methods vary to compute the modification and apply the modification. Through comprehensive empirical studies across machine translation, text translation summarization, language translation, and text classification benchmarks, we find the unified view classification benchmarks to identify important important design choices in previous design approaches. Furthermore, our unified framework enables the extension of design elements across different parametrations, and as a result we can instantiate new parameter exportable fine-tuning parameters that tune less parameters while being more effective than previous methods, achieving comparable results to fine-tuning all parameters on all four tasks.\n",
      "---------------\n",
      "Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the TPI parameters, which becomes prohibitive as the model size grows and the number of tasks grows. Recently proposed fee-efficient work-efficient transfer number of fine-tunil learning parameters that include a variety of small (extra) parameters to attain a successful performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter transfer understanding and present a unified framework for connecting connections between them. Specifically, we re-frame them as modifications to them in specific frames, and define them as pre-trained hidden mechanics in different pre-trained models, such as such a function to compute the modification and the modification to compute the position. Through comprehensive empirical studies across machine translation, text translation translation, text translation benchmark, and text classification benchmarks, we identify the unified view to design important concepts in design choices. Furthermore, our unified framework enables the extension of design elements across different approaches, and as a result we are able to create new parameter transfer learning fast-tunilned parameters that tune less fine-tunil results while being more effective than previous methods, achieving comparable results to fine-tunil all parameters on four different tasks.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# question = \"We saw that Reinforce worked well. \"\n",
    "# question = \"i am a handsome\"\n",
    "question = \"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the \"\n",
    "\n",
    "start_ids = encode(start + question)\n",
    "x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
    "print(x)\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\n",
    "\n",
    "            result = y[0].tolist()\n",
    "\n",
    "            eot_index = len(result)\n",
    "            for ci, c in enumerate(result):\n",
    "                if c == enc.eot_token:\n",
    "                    eot_index = ci\n",
    "                    break\n",
    "            \n",
    "            print(decode(result[:eot_index]))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 716, 257, 22665, 442, 3762, 2933, 13, 314, 2107]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i am a<|endoftext|> chinese boy. I live'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"i am a handsome chinese boy. I live\"\n",
    "\n",
    "start_ids = encode(start + question)\n",
    "\n",
    "print(start_ids)\n",
    "\n",
    "my_ids = [72, 716, 257, 22665, 442, 3762, 2933, 13, 314, 2107]\n",
    "\n",
    "decode(my_ids)\n",
    "\n",
    "my_ids = [72, 716, 257, enc.eot_token, 442, 3762, 2933, 13, 314, 2107]\n",
    "\n",
    "decode(my_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
