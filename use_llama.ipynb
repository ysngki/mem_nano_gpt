{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from my_modeling_llama import LlamaForCausalLM\n",
    "import torch\n",
    "from my_utils import get_seq_train_batch\n",
    "import numpy as np\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf4b58b12004f5993e0693029498d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_8bit=True, device_map={'': device}, torch_dtype=torch.float16, cache_dir=\"/data/yuanhang/hf_cache\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_8bit=True, device_map='auto', torch_dtype=torch.float16, cache_dir=\"/data/yuanhang/hf_cache\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"auto\")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.float32\n",
      "model.layers.0.self_attn.q_proj.weight torch.int8\n",
      "model.layers.0.self_attn.k_proj.weight torch.int8\n",
      "model.layers.0.self_attn.v_proj.weight torch.int8\n",
      "model.layers.0.self_attn.o_proj.weight torch.int8\n",
      "model.layers.0.mlp.gate_proj.weight torch.int8\n",
      "model.layers.0.mlp.down_proj.weight torch.int8\n",
      "model.layers.0.mlp.up_proj.weight torch.int8\n",
      "model.layers.0.input_layernorm.weight torch.float32\n",
      "model.layers.0.post_attention_layernorm.weight torch.float32\n",
      "model.layers.1.self_attn.q_proj.weight torch.int8\n",
      "model.layers.1.self_attn.k_proj.weight torch.int8\n",
      "model.layers.1.self_attn.v_proj.weight torch.int8\n",
      "model.layers.1.self_attn.o_proj.weight torch.int8\n",
      "model.layers.1.mlp.gate_proj.weight torch.int8\n",
      "model.layers.1.mlp.down_proj.weight torch.int8\n",
      "model.layers.1.mlp.up_proj.weight torch.int8\n",
      "model.layers.1.input_layernorm.weight torch.float32\n",
      "model.layers.1.post_attention_layernorm.weight torch.float32\n",
      "model.layers.2.self_attn.q_proj.weight torch.int8\n",
      "model.layers.2.self_attn.k_proj.weight torch.int8\n",
      "model.layers.2.self_attn.v_proj.weight torch.int8\n",
      "model.layers.2.self_attn.o_proj.weight torch.int8\n",
      "model.layers.2.mlp.gate_proj.weight torch.int8\n",
      "model.layers.2.mlp.down_proj.weight torch.int8\n",
      "model.layers.2.mlp.up_proj.weight torch.int8\n",
      "model.layers.2.input_layernorm.weight torch.float32\n",
      "model.layers.2.post_attention_layernorm.weight torch.float32\n",
      "model.layers.3.self_attn.q_proj.weight torch.int8\n",
      "model.layers.3.self_attn.k_proj.weight torch.int8\n",
      "model.layers.3.self_attn.v_proj.weight torch.int8\n",
      "model.layers.3.self_attn.o_proj.weight torch.int8\n",
      "model.layers.3.mlp.gate_proj.weight torch.int8\n",
      "model.layers.3.mlp.down_proj.weight torch.int8\n",
      "model.layers.3.mlp.up_proj.weight torch.int8\n",
      "model.layers.3.input_layernorm.weight torch.float32\n",
      "model.layers.3.post_attention_layernorm.weight torch.float32\n",
      "model.layers.4.self_attn.q_proj.weight torch.int8\n",
      "model.layers.4.self_attn.k_proj.weight torch.int8\n",
      "model.layers.4.self_attn.v_proj.weight torch.int8\n",
      "model.layers.4.self_attn.o_proj.weight torch.int8\n",
      "model.layers.4.mlp.gate_proj.weight torch.int8\n",
      "model.layers.4.mlp.down_proj.weight torch.int8\n",
      "model.layers.4.mlp.up_proj.weight torch.int8\n",
      "model.layers.4.input_layernorm.weight torch.float32\n",
      "model.layers.4.post_attention_layernorm.weight torch.float32\n",
      "model.layers.5.self_attn.q_proj.weight torch.int8\n",
      "model.layers.5.self_attn.k_proj.weight torch.int8\n",
      "model.layers.5.self_attn.v_proj.weight torch.int8\n",
      "model.layers.5.self_attn.o_proj.weight torch.int8\n",
      "model.layers.5.mlp.gate_proj.weight torch.int8\n",
      "model.layers.5.mlp.down_proj.weight torch.int8\n",
      "model.layers.5.mlp.up_proj.weight torch.int8\n",
      "model.layers.5.input_layernorm.weight torch.float32\n",
      "model.layers.5.post_attention_layernorm.weight torch.float32\n",
      "model.layers.6.self_attn.q_proj.weight torch.int8\n",
      "model.layers.6.self_attn.k_proj.weight torch.int8\n",
      "model.layers.6.self_attn.v_proj.weight torch.int8\n",
      "model.layers.6.self_attn.o_proj.weight torch.int8\n",
      "model.layers.6.mlp.gate_proj.weight torch.int8\n",
      "model.layers.6.mlp.down_proj.weight torch.int8\n",
      "model.layers.6.mlp.up_proj.weight torch.int8\n",
      "model.layers.6.input_layernorm.weight torch.float32\n",
      "model.layers.6.post_attention_layernorm.weight torch.float32\n",
      "model.layers.7.self_attn.q_proj.weight torch.int8\n",
      "model.layers.7.self_attn.k_proj.weight torch.int8\n",
      "model.layers.7.self_attn.v_proj.weight torch.int8\n",
      "model.layers.7.self_attn.o_proj.weight torch.int8\n",
      "model.layers.7.mlp.gate_proj.weight torch.int8\n",
      "model.layers.7.mlp.down_proj.weight torch.int8\n",
      "model.layers.7.mlp.up_proj.weight torch.int8\n",
      "model.layers.7.input_layernorm.weight torch.float32\n",
      "model.layers.7.post_attention_layernorm.weight torch.float32\n",
      "model.layers.8.self_attn.q_proj.weight torch.int8\n",
      "model.layers.8.self_attn.k_proj.weight torch.int8\n",
      "model.layers.8.self_attn.v_proj.weight torch.int8\n",
      "model.layers.8.self_attn.o_proj.weight torch.int8\n",
      "model.layers.8.mlp.gate_proj.weight torch.int8\n",
      "model.layers.8.mlp.down_proj.weight torch.int8\n",
      "model.layers.8.mlp.up_proj.weight torch.int8\n",
      "model.layers.8.input_layernorm.weight torch.float32\n",
      "model.layers.8.post_attention_layernorm.weight torch.float32\n",
      "model.layers.9.self_attn.q_proj.weight torch.int8\n",
      "model.layers.9.self_attn.k_proj.weight torch.int8\n",
      "model.layers.9.self_attn.v_proj.weight torch.int8\n",
      "model.layers.9.self_attn.o_proj.weight torch.int8\n",
      "model.layers.9.mlp.gate_proj.weight torch.int8\n",
      "model.layers.9.mlp.down_proj.weight torch.int8\n",
      "model.layers.9.mlp.up_proj.weight torch.int8\n",
      "model.layers.9.input_layernorm.weight torch.float32\n",
      "model.layers.9.post_attention_layernorm.weight torch.float32\n",
      "model.layers.10.self_attn.q_proj.weight torch.int8\n",
      "model.layers.10.self_attn.k_proj.weight torch.int8\n",
      "model.layers.10.self_attn.v_proj.weight torch.int8\n",
      "model.layers.10.self_attn.o_proj.weight torch.int8\n",
      "model.layers.10.mlp.gate_proj.weight torch.int8\n",
      "model.layers.10.mlp.down_proj.weight torch.int8\n",
      "model.layers.10.mlp.up_proj.weight torch.int8\n",
      "model.layers.10.input_layernorm.weight torch.float32\n",
      "model.layers.10.post_attention_layernorm.weight torch.float32\n",
      "model.layers.11.self_attn.q_proj.weight torch.int8\n",
      "model.layers.11.self_attn.k_proj.weight torch.int8\n",
      "model.layers.11.self_attn.v_proj.weight torch.int8\n",
      "model.layers.11.self_attn.o_proj.weight torch.int8\n",
      "model.layers.11.mlp.gate_proj.weight torch.int8\n",
      "model.layers.11.mlp.down_proj.weight torch.int8\n",
      "model.layers.11.mlp.up_proj.weight torch.int8\n",
      "model.layers.11.input_layernorm.weight torch.float32\n",
      "model.layers.11.post_attention_layernorm.weight torch.float32\n",
      "model.layers.12.self_attn.q_proj.weight torch.int8\n",
      "model.layers.12.self_attn.k_proj.weight torch.int8\n",
      "model.layers.12.self_attn.v_proj.weight torch.int8\n",
      "model.layers.12.self_attn.o_proj.weight torch.int8\n",
      "model.layers.12.mlp.gate_proj.weight torch.int8\n",
      "model.layers.12.mlp.down_proj.weight torch.int8\n",
      "model.layers.12.mlp.up_proj.weight torch.int8\n",
      "model.layers.12.input_layernorm.weight torch.float32\n",
      "model.layers.12.post_attention_layernorm.weight torch.float32\n",
      "model.layers.13.self_attn.q_proj.weight torch.int8\n",
      "model.layers.13.self_attn.k_proj.weight torch.int8\n",
      "model.layers.13.self_attn.v_proj.weight torch.int8\n",
      "model.layers.13.self_attn.o_proj.weight torch.int8\n",
      "model.layers.13.mlp.gate_proj.weight torch.int8\n",
      "model.layers.13.mlp.down_proj.weight torch.int8\n",
      "model.layers.13.mlp.up_proj.weight torch.int8\n",
      "model.layers.13.input_layernorm.weight torch.float32\n",
      "model.layers.13.post_attention_layernorm.weight torch.float32\n",
      "model.layers.14.self_attn.q_proj.weight torch.int8\n",
      "model.layers.14.self_attn.k_proj.weight torch.int8\n",
      "model.layers.14.self_attn.v_proj.weight torch.int8\n",
      "model.layers.14.self_attn.o_proj.weight torch.int8\n",
      "model.layers.14.mlp.gate_proj.weight torch.int8\n",
      "model.layers.14.mlp.down_proj.weight torch.int8\n",
      "model.layers.14.mlp.up_proj.weight torch.int8\n",
      "model.layers.14.input_layernorm.weight torch.float32\n",
      "model.layers.14.post_attention_layernorm.weight torch.float32\n",
      "model.layers.15.self_attn.q_proj.weight torch.int8\n",
      "model.layers.15.self_attn.k_proj.weight torch.int8\n",
      "model.layers.15.self_attn.v_proj.weight torch.int8\n",
      "model.layers.15.self_attn.o_proj.weight torch.int8\n",
      "model.layers.15.mlp.gate_proj.weight torch.int8\n",
      "model.layers.15.mlp.down_proj.weight torch.int8\n",
      "model.layers.15.mlp.up_proj.weight torch.int8\n",
      "model.layers.15.input_layernorm.weight torch.float32\n",
      "model.layers.15.post_attention_layernorm.weight torch.float32\n",
      "model.layers.16.self_attn.q_proj.weight torch.int8\n",
      "model.layers.16.self_attn.k_proj.weight torch.int8\n",
      "model.layers.16.self_attn.v_proj.weight torch.int8\n",
      "model.layers.16.self_attn.o_proj.weight torch.int8\n",
      "model.layers.16.mlp.gate_proj.weight torch.int8\n",
      "model.layers.16.mlp.down_proj.weight torch.int8\n",
      "model.layers.16.mlp.up_proj.weight torch.int8\n",
      "model.layers.16.input_layernorm.weight torch.float32\n",
      "model.layers.16.post_attention_layernorm.weight torch.float32\n",
      "model.layers.17.self_attn.q_proj.weight torch.int8\n",
      "model.layers.17.self_attn.k_proj.weight torch.int8\n",
      "model.layers.17.self_attn.v_proj.weight torch.int8\n",
      "model.layers.17.self_attn.o_proj.weight torch.int8\n",
      "model.layers.17.mlp.gate_proj.weight torch.int8\n",
      "model.layers.17.mlp.down_proj.weight torch.int8\n",
      "model.layers.17.mlp.up_proj.weight torch.int8\n",
      "model.layers.17.input_layernorm.weight torch.float32\n",
      "model.layers.17.post_attention_layernorm.weight torch.float32\n",
      "model.layers.18.self_attn.q_proj.weight torch.int8\n",
      "model.layers.18.self_attn.k_proj.weight torch.int8\n",
      "model.layers.18.self_attn.v_proj.weight torch.int8\n",
      "model.layers.18.self_attn.o_proj.weight torch.int8\n",
      "model.layers.18.mlp.gate_proj.weight torch.int8\n",
      "model.layers.18.mlp.down_proj.weight torch.int8\n",
      "model.layers.18.mlp.up_proj.weight torch.int8\n",
      "model.layers.18.input_layernorm.weight torch.float32\n",
      "model.layers.18.post_attention_layernorm.weight torch.float32\n",
      "model.layers.19.self_attn.q_proj.weight torch.int8\n",
      "model.layers.19.self_attn.k_proj.weight torch.int8\n",
      "model.layers.19.self_attn.v_proj.weight torch.int8\n",
      "model.layers.19.self_attn.o_proj.weight torch.int8\n",
      "model.layers.19.mlp.gate_proj.weight torch.int8\n",
      "model.layers.19.mlp.down_proj.weight torch.int8\n",
      "model.layers.19.mlp.up_proj.weight torch.int8\n",
      "model.layers.19.input_layernorm.weight torch.float32\n",
      "model.layers.19.post_attention_layernorm.weight torch.float32\n",
      "model.layers.20.self_attn.q_proj.weight torch.int8\n",
      "model.layers.20.self_attn.k_proj.weight torch.int8\n",
      "model.layers.20.self_attn.v_proj.weight torch.int8\n",
      "model.layers.20.self_attn.o_proj.weight torch.int8\n",
      "model.layers.20.mlp.gate_proj.weight torch.int8\n",
      "model.layers.20.mlp.down_proj.weight torch.int8\n",
      "model.layers.20.mlp.up_proj.weight torch.int8\n",
      "model.layers.20.input_layernorm.weight torch.float32\n",
      "model.layers.20.post_attention_layernorm.weight torch.float32\n",
      "model.layers.21.self_attn.q_proj.weight torch.int8\n",
      "model.layers.21.self_attn.k_proj.weight torch.int8\n",
      "model.layers.21.self_attn.v_proj.weight torch.int8\n",
      "model.layers.21.self_attn.o_proj.weight torch.int8\n",
      "model.layers.21.mlp.gate_proj.weight torch.int8\n",
      "model.layers.21.mlp.down_proj.weight torch.int8\n",
      "model.layers.21.mlp.up_proj.weight torch.int8\n",
      "model.layers.21.input_layernorm.weight torch.float32\n",
      "model.layers.21.post_attention_layernorm.weight torch.float32\n",
      "model.layers.22.self_attn.q_proj.weight torch.int8\n",
      "model.layers.22.self_attn.k_proj.weight torch.int8\n",
      "model.layers.22.self_attn.v_proj.weight torch.int8\n",
      "model.layers.22.self_attn.o_proj.weight torch.int8\n",
      "model.layers.22.mlp.gate_proj.weight torch.int8\n",
      "model.layers.22.mlp.down_proj.weight torch.int8\n",
      "model.layers.22.mlp.up_proj.weight torch.int8\n",
      "model.layers.22.input_layernorm.weight torch.float32\n",
      "model.layers.22.post_attention_layernorm.weight torch.float32\n",
      "model.layers.23.self_attn.q_proj.weight torch.int8\n",
      "model.layers.23.self_attn.k_proj.weight torch.int8\n",
      "model.layers.23.self_attn.v_proj.weight torch.int8\n",
      "model.layers.23.self_attn.o_proj.weight torch.int8\n",
      "model.layers.23.mlp.gate_proj.weight torch.int8\n",
      "model.layers.23.mlp.down_proj.weight torch.int8\n",
      "model.layers.23.mlp.up_proj.weight torch.int8\n",
      "model.layers.23.input_layernorm.weight torch.float32\n",
      "model.layers.23.post_attention_layernorm.weight torch.float32\n",
      "model.layers.24.self_attn.q_proj.weight torch.int8\n",
      "model.layers.24.self_attn.k_proj.weight torch.int8\n",
      "model.layers.24.self_attn.v_proj.weight torch.int8\n",
      "model.layers.24.self_attn.o_proj.weight torch.int8\n",
      "model.layers.24.mlp.gate_proj.weight torch.int8\n",
      "model.layers.24.mlp.down_proj.weight torch.int8\n",
      "model.layers.24.mlp.up_proj.weight torch.int8\n",
      "model.layers.24.input_layernorm.weight torch.float32\n",
      "model.layers.24.post_attention_layernorm.weight torch.float32\n",
      "model.layers.25.self_attn.q_proj.weight torch.int8\n",
      "model.layers.25.self_attn.k_proj.weight torch.int8\n",
      "model.layers.25.self_attn.v_proj.weight torch.int8\n",
      "model.layers.25.self_attn.o_proj.weight torch.int8\n",
      "model.layers.25.mlp.gate_proj.weight torch.int8\n",
      "model.layers.25.mlp.down_proj.weight torch.int8\n",
      "model.layers.25.mlp.up_proj.weight torch.int8\n",
      "model.layers.25.input_layernorm.weight torch.float32\n",
      "model.layers.25.post_attention_layernorm.weight torch.float32\n",
      "model.layers.26.self_attn.q_proj.weight torch.int8\n",
      "model.layers.26.self_attn.k_proj.weight torch.int8\n",
      "model.layers.26.self_attn.v_proj.weight torch.int8\n",
      "model.layers.26.self_attn.o_proj.weight torch.int8\n",
      "model.layers.26.mlp.gate_proj.weight torch.int8\n",
      "model.layers.26.mlp.down_proj.weight torch.int8\n",
      "model.layers.26.mlp.up_proj.weight torch.int8\n",
      "model.layers.26.input_layernorm.weight torch.float32\n",
      "model.layers.26.post_attention_layernorm.weight torch.float32\n",
      "model.layers.27.self_attn.q_proj.weight torch.int8\n",
      "model.layers.27.self_attn.k_proj.weight torch.int8\n",
      "model.layers.27.self_attn.v_proj.weight torch.int8\n",
      "model.layers.27.self_attn.o_proj.weight torch.int8\n",
      "model.layers.27.mlp.gate_proj.weight torch.int8\n",
      "model.layers.27.mlp.down_proj.weight torch.int8\n",
      "model.layers.27.mlp.up_proj.weight torch.int8\n",
      "model.layers.27.input_layernorm.weight torch.float32\n",
      "model.layers.27.post_attention_layernorm.weight torch.float32\n",
      "model.layers.28.self_attn.q_proj.weight torch.int8\n",
      "model.layers.28.self_attn.k_proj.weight torch.int8\n",
      "model.layers.28.self_attn.v_proj.weight torch.int8\n",
      "model.layers.28.self_attn.o_proj.weight torch.int8\n",
      "model.layers.28.mlp.gate_proj.weight torch.int8\n",
      "model.layers.28.mlp.down_proj.weight torch.int8\n",
      "model.layers.28.mlp.up_proj.weight torch.int8\n",
      "model.layers.28.input_layernorm.weight torch.float32\n",
      "model.layers.28.post_attention_layernorm.weight torch.float32\n",
      "model.layers.29.self_attn.q_proj.weight torch.int8\n",
      "model.layers.29.self_attn.k_proj.weight torch.int8\n",
      "model.layers.29.self_attn.v_proj.weight torch.int8\n",
      "model.layers.29.self_attn.o_proj.weight torch.int8\n",
      "model.layers.29.mlp.gate_proj.weight torch.int8\n",
      "model.layers.29.mlp.down_proj.weight torch.int8\n",
      "model.layers.29.mlp.up_proj.weight torch.int8\n",
      "model.layers.29.input_layernorm.weight torch.float32\n",
      "model.layers.29.post_attention_layernorm.weight torch.float32\n",
      "model.layers.30.self_attn.q_proj.weight torch.int8\n",
      "model.layers.30.self_attn.k_proj.weight torch.int8\n",
      "model.layers.30.self_attn.v_proj.weight torch.int8\n",
      "model.layers.30.self_attn.o_proj.weight torch.int8\n",
      "model.layers.30.mlp.gate_proj.weight torch.int8\n",
      "model.layers.30.mlp.down_proj.weight torch.int8\n",
      "model.layers.30.mlp.up_proj.weight torch.int8\n",
      "model.layers.30.input_layernorm.weight torch.float32\n",
      "model.layers.30.post_attention_layernorm.weight torch.float32\n",
      "model.layers.31.self_attn.q_proj.weight torch.int8\n",
      "model.layers.31.self_attn.k_proj.weight torch.int8\n",
      "model.layers.31.self_attn.v_proj.weight torch.int8\n",
      "model.layers.31.self_attn.o_proj.weight torch.int8\n",
      "model.layers.31.mlp.gate_proj.weight torch.int8\n",
      "model.layers.31.mlp.down_proj.weight torch.int8\n",
      "model.layers.31.mlp.up_proj.weight torch.int8\n",
      "model.layers.31.input_layernorm.weight torch.float32\n",
      "model.layers.31.post_attention_layernorm.weight torch.float32\n",
      "model.norm.weight torch.float32\n",
      "lm_head.weight torch.float32\n"
     ]
    }
   ],
   "source": [
    "for n, p in model.named_parameters():\n",
    "    print(n, p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_8bit\": true\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.29.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "tokenizer.pad_token = model.model.padding_idx\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,  343,  574, 8071]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"yangyy\", return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(question, input_parameter=None):\n",
    "    x = tokenizer(question, return_tensors=\"pt\", padding=True)\n",
    "    x.to(device)\n",
    "    print(x.input_ids.shape)\n",
    "    # print(x)\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(1):\n",
    "                # y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\n",
    "                y = model.generate(x.input_ids, max_length=512)\n",
    "\n",
    "                y = y[:, x.input_ids.shape[1]:]\n",
    "\n",
    "                result = tokenizer.batch_decode(y, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                \n",
    "                print(result)\n",
    "                print('---------------')\n",
    "            print('===============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 264])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A \\nQuestion: According to Kant, the moral law is _____. \\n A: a set of rules that are binding on all rational beings. B: a set of rules that are binding on all human beings. C: a set of rules that are binding on all living beings. D: a set of rules that are binding on all sentient beings. \\n Answer: A \\nQuestion: According to Kant, the moral law is _____. \\n A: a set of rules that are binding on all rational beings. B: a set of rules that are binding on all human beings. C: a set of rules that are binding on all living beings. D: a set of rules that are binding on all sentient beings. \\n Answer: A \\nQuestion: According to Kant, the moral law is _____. \\n A: a set of rules that are binding on all rational beings. B: a set of rules that are binding on all human beings. C: a set of rules that are binding on all living beings. D: a set of rules that are binding on all sentient beings. \\n Answer']\n",
      "---------------\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "question = [\n",
    "    'Question: Aesthetics deals with objects that are_____. \\n A: essential to our existence B: unimportant to most people C: not essential to our existence D: rarely viewed. \\n Answer: C \\n',\n",
    "    'Question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____. \\n A: the state B: the justice system C: the body D: the soul. \\n Answer: D \\n',\n",
    "    'Question: For Socrates, the soul is harmed by lack of _____. \\n A: knowledge B: wealth C: community D: courage. \\n Answer: A \\n',\n",
    "    'Question: According to Kant, nothing can be called “good” without qualification except _____. \\n A: right action B: good consequences C: happiness D: a good will. \\n Answer: D \\n',\n",
    "    'Question: Baier argues that genuine moral rules: \\n A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \\n Answer:',\n",
    "    # \"Question: Plato's view is that true beauty is _____. \\n A: found in everyday objects B: nonexistent C: everywhere in the natural world D: not of this world. \\n \",\n",
    "    ]\n",
    "\n",
    "question_str = \"\"\n",
    "for q in question:\n",
    "    question_str += q\n",
    "\n",
    "generate_sentence([question_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb1f9d646084956a05528ef4179775e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# other_model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_8bit=True, device_map='auto', torch_dtype=torch.float16, cache_dir=\"/data/yuanhang/hf_cache\")\n",
    "# other_model = prepare_model_for_kbit_training(other_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2721, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "torch.Size([16, 256, 4096]) tensor([[ 2890,   399, 24071,  ...,     0,     0,     0],\n",
      "        [ 4123, 16823,   515,  ...,     0,     0,     0],\n",
      "        [ 9358,   293, 10672,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  297,  4958,   310,  ...,     0,     0,     0],\n",
      "        [ 1693,  4250,  3304,  ...,     0,     0,     0],\n",
      "        [  338,   278,  1302,  ...,     0,     0,     0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_data = np.memmap(\"./data/llama_openwebtext/train.bin\", dtype=np.uint16, mode='r')\n",
    "\n",
    "for _ in range(20):\n",
    "    with torch.no_grad():\n",
    "        data_pointer, x, y, attention_mask, seg_length_list = get_seq_train_batch(train_data, [random.randint(0, 100000000)], 16, 256, 128, device, device_type, False)\n",
    "\n",
    "        x = x.squeeze(0)\n",
    "        y = y.squeeze(0)\n",
    "        attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "        # print(x[0])\n",
    "        # print(y[0])\n",
    "\n",
    "        output = model(x, attention_mask=attention_mask, labels=y)\n",
    "        print(output.loss)\n",
    "\n",
    "        # output = other_model(x, attention_mask=attention_mask, labels=y)\n",
    "        # print(output.loss)\n",
    "        \n",
    "        if torch.isnan(output.loss):\n",
    "            print()\n",
    "        print(\"---\"*20)\n",
    "\n",
    "        out = model(x, output_embeds=True)\n",
    "        print(out.shape, x)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4192, device='cuda:1')\n",
      "tensor(1.4192, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shift_logits = output.logits.view(-1, model.config.vocab_size)\n",
    "shift_labels = y.view(-1)\n",
    "\n",
    "loss_fct = CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "loss = loss_fct(shift_logits, shift_labels)\n",
    "print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
