{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-14 18:22:20,295] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from my_modeling_llama import LlamaForCausalLM\n",
    "import torch\n",
    "from my_utils import get_seq_train_batch\n",
    "import numpy as np\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b68d20df6f4999802cbbae833218e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_8bit=True, device_map={'': device}, torch_dtype=torch.float16, cache_dir=\"/data/yuanhang/hf_cache\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", load_in_8bit=True, device_map={'': \"cuda:0\"}, torch_dtype=torch.float16, cache_dir=\"/data/yuanhang/hf_cache\")\n",
    "# model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"auto\")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in model.named_parameters():\n",
    "#     print(n, p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "tokenizer.pad_token = model.model.padding_idx\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,  343,  574, 8071]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"yangyy\", return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(question, input_parameter=None, peft=\"prompt\"):\n",
    "    x = tokenizer(question, return_tensors=\"pt\", padding=True)\n",
    "    x.to(device)\n",
    "    print(x.input_ids.shape)\n",
    "    # print(x)\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(1):\n",
    "                # y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\n",
    "                y = model.generate(x.input_ids, max_length=512, input_parameter=input_parameter, peft=peft)\n",
    "\n",
    "                y = y[:, x.input_ids.shape[1]:]\n",
    "\n",
    "                result = tokenizer.batch_decode(y, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                \n",
    "                print(result[0])\n",
    "                print('---------------')\n",
    "            print('===============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 264])\n",
      "B \n",
      "Question: Kant argues that the moral law can be known _____. \n",
      " A: by intuition B: by experience C: by reason D: by revelation. \n",
      " Answer: A \n",
      "Question: For Kant, the moral law is not something we _____. \n",
      " A: learn from the Bible B: know from experience C: discover through intuition D: can choose to follow or not. \n",
      " Answer: C \n",
      "Question: According to Kant, the moral law is _____. \n",
      " A: a law of nature B: a law of nature that is also a law of reason C: a law of nature that is also a law of God D: a law of reason that is also a law of God. \n",
      " Answer: B \n",
      "Question: Kant argues that we are not free to choose to follow the moral law because it is _____. \n",
      " A: a law of nature B: a law of God C: a law of reason D: a law of experience. \n",
      " Answer: B \n",
      "Question: Kant argues that we cannot choose to follow the moral law because it is _____. \n",
      " A: a law of\n",
      "---------------\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "question = [\n",
    "    'Question: Aesthetics deals with objects that are_____. \\n A: essential to our existence B: unimportant to most people C: not essential to our existence D: rarely viewed. \\n Answer: C \\n',\n",
    "    'Question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____. \\n A: the state B: the justice system C: the body D: the soul. \\n Answer: D \\n',\n",
    "    'Question: For Socrates, the soul is harmed by lack of _____. \\n A: knowledge B: wealth C: community D: courage. \\n Answer: A \\n',\n",
    "    'Question: According to Kant, nothing can be called “good” without qualification except _____. \\n A: right action B: good consequences C: happiness D: a good will. \\n Answer: D \\n',\n",
    "    'Question: Baier argues that genuine moral rules: \\n A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \\n Answer:',\n",
    "    # \"Question: Plato's view is that true beauty is _____. \\n A: found in everyday objects B: nonexistent C: everywhere in the natural world D: not of this world. \\n \",\n",
    "    ]\n",
    "\n",
    "question_str = \"\"\n",
    "for q in question:\n",
    "    question_str += q\n",
    "\n",
    "generate_sentence([question_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2665, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_data = np.memmap(\"./data/llama_openwebtext/train.bin\", dtype=np.uint16, mode='r')\n",
    "\n",
    "for _ in range(20):\n",
    "    with torch.no_grad():\n",
    "        data_pointer, x, y, attention_mask, seg_length_list = get_seq_train_batch(train_data, [random.randint(0, 100000000)], 16, 256, 128, device, device_type, False)\n",
    "\n",
    "        x = x.squeeze(0)\n",
    "        y = y.squeeze(0)\n",
    "        attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "        # print(x[0])\n",
    "        # print(y[0])\n",
    "\n",
    "        output = model(x, attention_mask=attention_mask, labels=y)\n",
    "        print(output.loss)\n",
    "\n",
    "        break\n",
    "\n",
    "        # output = other_model(x, attention_mask=attention_mask, labels=y)\n",
    "        # print(output.loss)\n",
    "        \n",
    "        # if torch.isnan(output.loss):\n",
    "        #     print(\"nan!!!\")\n",
    "        # print(\"---\"*20)\n",
    "\n",
    "        # out = model(x, output_embeds=True)\n",
    "        # print(out.shape, x)\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4192, device='cuda:1')\n",
      "tensor(1.4192, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shift_logits = output.logits.view(-1, model.config.vocab_size)\n",
    "shift_labels = y.view(-1)\n",
    "\n",
    "loss_fct = CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "loss = loss_fct(shift_logits, shift_labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load memory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1342.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MemoryRobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (key): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (value): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "          (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conversion_dense): Linear(in_features=4096, out_features=131072, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_modeling_roberta import MemoryRobertaModel\n",
    "\n",
    "out_dir = 'out'\n",
    "\n",
    "peft_method = \"prompt\"\n",
    "# peft_method = \"lora\"\n",
    "\n",
    "ckpt_path = os.path.join(out_dir, 'predict_1st_kl_1seg_llama.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "evolver_config = checkpoint['evolver_config']\n",
    "evolver_model = MemoryRobertaModel(evolver_config)\n",
    "state_dict = checkpoint['evolver_model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "evolver_model.load_state_dict(state_dict)\n",
    "\n",
    "evolver_model.eval()\n",
    "evolver_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"iter_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parameter(context_list=None, context_id_list=None):\n",
    "    if context_list is not None:\n",
    "        encoded_context = []\n",
    "\n",
    "        for c in context_list:\n",
    "            ids = tokenizer(c, return_tensors=\"pt\", padding=True)\n",
    "            # ids.append(enc.eot_token)\n",
    "            encoded_context.append(ids.input_ids.to(device))\n",
    "    \n",
    "    if context_id_list is not None:\n",
    "        encoded_context = context_id_list\n",
    "        \n",
    "    input_memory = None\n",
    "    target_model_parameter = None\n",
    "\n",
    "    for index, ec in enumerate(encoded_context):\n",
    "        output_embeds = model(input_ids=ec, output_embeds=True, return_dict=False)\n",
    "\n",
    "        input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=input_memory)[\"memory_output\"]\n",
    "\n",
    "    # last memory -> X\n",
    "    target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "\n",
    "    # print(input_memory)\n",
    "    # print(target_model_parameter.shape)\n",
    "    \n",
    "    return target_model_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 73])\n",
      "B: make take into account the interests of all sentient beings. \n",
      "Explanation: \n",
      "B: The only moral rule that makes sense is one that takes into account the interests of all sentient beings.\n",
      "A: This is not a moral rule.\n",
      "C: This is not a moral rule.\n",
      "D: This is not a moral rule.\n",
      "\n",
      "## Solution\n",
      "\n",
      "B: The only moral rule that makes sense is one that takes into account the interests of all sentient beings.\n",
      "---------------\n",
      "===============================================================\n",
      "torch.Size([1, 73])\n",
      "A\n",
      "Explanation:\n",
      "The question is asking you to find the best argument for a conclusion.\n",
      "The conclusion is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must be for the good of human beings.\n",
      "The argument is that moral rules must take into account the interests of all sentient beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all sentient beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "The argument is that moral rules must take into account the interests of all living beings.\n",
      "---------------\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "context = [\n",
    "    'Question: Aesthetics deals with objects that are_____. \\n A: essential to our existence B: unimportant to most people C: not essential to our existence D: rarely viewed. \\n Answer: C \\n',\n",
    "    'Question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____. \\n A: the state B: the justice system C: the body D: the soul. \\n Answer: D \\n',\n",
    "    'Question: For Socrates, the soul is harmed by lack of _____. \\n A: knowledge B: wealth C: community D: courage. \\n Answer: A \\n',\n",
    "    'Question: According to Kant, nothing can be called “good” without qualification except _____. \\n A: right action B: good consequences C: happiness D: a good will. \\n Answer: D \\n',\n",
    "    # \"Question: Plato's view is that true beauty is _____. \\n A: found in everyday objects B: nonexistent C: everywhere in the natural world D: not of this world. \\n \",\n",
    "    ]\n",
    "\n",
    "context_str = \"\"\n",
    "for c in context:\n",
    "    context_str += q\n",
    "\n",
    "question = 'Question: Baier argues that genuine moral rules: \\n A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \\n Answer:',\n",
    "\n",
    "input_parameter = generate_parameter(context_list=[context])\n",
    "\n",
    "generate_sentence(question, input_parameter, peft_method)\n",
    "generate_sentence(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m The first name of the current US president is \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      4\u001b[0m input_parameter \u001b[39m=\u001b[39m generate_parameter(context_list\u001b[39m=\u001b[39mcontext)\n\u001b[0;32m----> 6\u001b[0m generate_sentence(question, input_parameter, peft_method)\n\u001b[1;32m      7\u001b[0m generate_sentence(question)\n",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m, in \u001b[0;36mgenerate_sentence\u001b[0;34m(question, input_parameter, peft)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mwith\u001b[39;00m ctx:\n\u001b[1;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m     11\u001b[0m         \u001b[39m# y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m         y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(x\u001b[39m.\u001b[39;49minput_ids, max_length\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, input_parameter\u001b[39m=\u001b[39;49minput_parameter, peft\u001b[39m=\u001b[39;49mpeft)\n\u001b[1;32m     14\u001b[0m         y \u001b[39m=\u001b[39m y[:, x\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:]\n\u001b[1;32m     16\u001b[0m         result \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(y, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/transformers/generation/utils.py:1565\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1558\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1559\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1560\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1561\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1562\u001b[0m     )\n\u001b[1;32m   1564\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1565\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1566\u001b[0m         input_ids,\n\u001b[1;32m   1567\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1568\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1569\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1570\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1571\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1572\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1573\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1574\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1575\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1576\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1577\u001b[0m     )\n\u001b[1;32m   1579\u001b[0m \u001b[39melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1580\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m generation_config\u001b[39m.\u001b[39mnum_beams:\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/transformers/generation/utils.py:2612\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2609\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2611\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2612\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2613\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2614\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2615\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2616\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2617\u001b[0m )\n\u001b[1;32m   2619\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2620\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/data/yuanhang/nanoGPT/my_modeling_llama.py:426\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, input_parameter, output_embeds, peft)\u001b[0m\n\u001b[1;32m    423\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    425\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    427\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    428\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    429\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    430\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    431\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    432\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    433\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    434\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    435\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    436\u001b[0m     input_parameter\u001b[39m=\u001b[39;49minput_parameter,\n\u001b[1;32m    437\u001b[0m     peft\u001b[39m=\u001b[39;49mpeft,\n\u001b[1;32m    438\u001b[0m )\n\u001b[1;32m    440\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    441\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/data/yuanhang/nanoGPT/my_modeling_llama.py:306\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, input_parameter, peft)\u001b[0m\n\u001b[1;32m    298\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    299\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    300\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    307\u001b[0m         hidden_states,\n\u001b[1;32m    308\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    309\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    310\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    311\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    312\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    315\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m memory_len \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m peft\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:293\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    290\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    292\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    294\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    295\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    296\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    297\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    298\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    299\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    300\u001b[0m )\n\u001b[1;32m    301\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    303\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:204\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     kv_seq_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m past_key_value[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m--> 204\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrotary_emb(value_states, seq_len\u001b[39m=\u001b[39mkv_seq_len)\n\u001b[1;32m    205\u001b[0m query_states, key_states \u001b[39m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[1;32m    206\u001b[0m \u001b[39m# [bsz, nh, t, hd]\u001b[39;00m\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "context = [\"Joe Biden is the current president of the United States of America.\"]\n",
    "question = ' The first name of the current US president is \"'\n",
    "\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "\n",
    "generate_sentence(question, input_parameter, peft_method)\n",
    "generate_sentence(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7204, device='cuda:0')\n",
      "tensor(2.5149, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.4255, device='cuda:0')\n",
      "tensor(2.3464, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(1.8927, device='cuda:0')\n",
      "tensor(1.8095, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.7253, device='cuda:0')\n",
      "tensor(2.5665, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.1516, device='cuda:0')\n",
      "tensor(1.9360, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.7536, device='cuda:0')\n",
      "tensor(2.5290, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.3158, device='cuda:0')\n",
      "tensor(2.1878, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.4065, device='cuda:0')\n",
      "tensor(2.1413, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.2744, device='cuda:0')\n",
      "tensor(2.1364, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(1.8700, device='cuda:0')\n",
      "tensor(1.7909, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.2895, device='cuda:0')\n",
      "tensor(2.1948, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.0684, device='cuda:0')\n",
      "tensor(1.8518, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(1.5323, device='cuda:0')\n",
      "tensor(1.5203, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(3.2276, device='cuda:0')\n",
      "tensor(2.8851, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.6920, device='cuda:0')\n",
      "tensor(2.5861, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(1.6709, device='cuda:0')\n",
      "tensor(1.5670, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.3100, device='cuda:0')\n",
      "tensor(2.2560, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.3152, device='cuda:0')\n",
      "tensor(2.2408, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.3079, device='cuda:0')\n",
      "tensor(2.1937, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(3.1157, device='cuda:0')\n",
      "tensor(3.1007, device='cuda:0')\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_data = np.memmap(\"./data/llama_openwebtext/train.bin\", dtype=np.uint16, mode='r')\n",
    "\n",
    "for _ in range(20):\n",
    "    with torch.no_grad():\n",
    "        data_pointer, x, y, attention_mask, seg_length_list = get_seq_train_batch(train_data, [random.randint(0, 100000000)], 16, 256, 128, device, device_type, False)\n",
    "\n",
    "        context = x.squeeze(0)[0:1]\n",
    "\n",
    "        x = x.squeeze(0)[1:2]\n",
    "        y = y.squeeze(0)[1:2]\n",
    "        attention_mask = attention_mask.squeeze(0)[1:2]\n",
    "\n",
    "        output_embeds = model(input_ids=context, output_embeds=True, return_dict=False)\n",
    "        input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=None)[\"memory_output\"]\n",
    "        target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "\n",
    "        # print(x[0])\n",
    "        # print(y[0])\n",
    "\n",
    "        output = model(x, attention_mask=attention_mask, labels=y)\n",
    "        print(output.loss)\n",
    "\n",
    "        output = model(x, attention_mask=attention_mask, labels=y, input_parameter=input_parameter, peft=\"prompt\")\n",
    "        print(output.loss)\n",
    "\n",
    "        # output = other_model(x, attention_mask=attention_mask, labels=y)\n",
    "        # print(output.loss)\n",
    "        \n",
    "        # if torch.isnan(output.loss):\n",
    "        #     print(\"nan!!!\")\n",
    "        print(\"---\"*20)\n",
    "\n",
    "        # out = model(x, output_embeds=True)\n",
    "        # print(out.shape, x)\n",
    "        # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
