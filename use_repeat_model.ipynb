{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /data/yuanhang/anaconda3/envs/moe/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import MemoryGPT as GPT\n",
    "import random\n",
    "\n",
    "from my_configuration_roberta import MemoryRobertaConfig\n",
    "from my_modeling_roberta import MemoryRobertaModel\n",
    "\n",
    "os.environ['TIKTOKEN_CACHE_DIR']=\"/data/yuanhang/tiktoken_cache_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 2 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "\n",
    "device = 'cuda:0' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "# device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "[27, 91, 437, 1659, 5239, 91, 29]\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = 'gpt2'\n",
    "\n",
    "# load pretrained model\n",
    "if \"gpt\" in pretrained_model:\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {pretrained_model}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    model = GPT.from_pretrained(pretrained_model, dict(dropout=0.0))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # backbone forzen\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    \n",
    "    pretrained_model_config = model.config\n",
    "else:\n",
    "    raise Exception(f\"Unrecognized pretrained model {pretrained_model}\")\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# ok let's assume gpt-2 encodings by default\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "# encode = lambda s: enc.encode_ordinary(s)\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "    \n",
    "print(enc.encode_ordinary(\"<|endoftext|>\"))\n",
    "print(enc.encode(\"<|endoftext|>\",  allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load memory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 49.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MemoryRobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conversion_dense): Linear(in_features=768, out_features=9216, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'repeat_1st_batch_4_seg.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "evolver_config = checkpoint['evolver_config']\n",
    "evolver_model = MemoryRobertaModel(evolver_config)\n",
    "state_dict = checkpoint['evolver_model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "evolver_model.load_state_dict(state_dict)\n",
    "\n",
    "evolver_model.eval()\n",
    "evolver_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_sentence(question, input_parameter=None):\n",
    "    start_ids = encode(question)\n",
    "    x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
    "    print(x)\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(1):\n",
    "                # y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\n",
    "                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=1, input_parameter=input_parameter)\n",
    "\n",
    "                result = y[0].tolist()[:200]\n",
    "\n",
    "                eot_index = len(result)\n",
    "                for ci, c in enumerate(result):\n",
    "                    if c == enc.eot_token:\n",
    "                        eot_index = ci\n",
    "                        break\n",
    "                \n",
    "                print(decode(result[:eot_index]))\n",
    "                print('---------------')\n",
    "            print('===============================================================')\n",
    "\n",
    "\n",
    "def generate_parameter(context_list=None, context_id_list=None, run_num=1, memory_lr=1.0):\n",
    "    if context_list is not None:\n",
    "        encoded_context = []\n",
    "\n",
    "        for c in context_list:\n",
    "            if isinstance(c, str):\n",
    "                ids = encode(c)\n",
    "            else:\n",
    "                ids = c.astype(np.int64)\n",
    "            # ids.append(enc.eot_token)\n",
    "            encoded_context.append(torch.tensor(ids)[None, ...].to(device))\n",
    "        \n",
    "    if context_id_list is not None:\n",
    "        encoded_context = context_id_list\n",
    "        \n",
    "    input_memory = None\n",
    "    target_model_parameter = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(run_num):\n",
    "            for index, ec in enumerate(encoded_context):\n",
    "                output_embeds = model(idx=ec, output_embeds=True)\n",
    "\n",
    "                input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=input_memory)[\"memory_output\"]\n",
    "\n",
    "    # last memory -> X\n",
    "    target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "\n",
    "    # print(input_memory)\n",
    "    # print(target_model_parameter.shape)\n",
    "    \n",
    "    return target_model_parameter\n",
    "\n",
    "\n",
    "def generate_batch_parameter(context_list=None, context_id_list=None, run_num=1, memory_lr=1.0):\n",
    "    if context_list is not None:\n",
    "        encoded_context_list = []\n",
    "\n",
    "        for c in context_list:\n",
    "            if isinstance(c, str):\n",
    "                ids = encode(c)\n",
    "            else:\n",
    "                ids = c.astype(np.int64)\n",
    "            # ids.append(enc.eot_token)\n",
    "            encoded_context_list.append(torch.tensor(ids)[None, ...].to(device))\n",
    "        \n",
    "        encoded_context_list = [torch.cat(encoded_context_list, dim=0)]\n",
    "    \n",
    "    if context_id_list is not None:\n",
    "        encoded_context_list = context_id_list\n",
    "    \n",
    "    input_memory = evolver_model.initial_memory\n",
    "    target_model_parameter = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(run_num):\n",
    "            for index, ec in enumerate(encoded_context_list):\n",
    "                batch_input_memory = input_memory.repeat(encoded_context_list[0].shape[0], 1, 1)\n",
    "\n",
    "                output_embeds = model(idx=ec, output_embeds=True)\n",
    "\n",
    "                new_batch_input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=batch_input_memory)[\"memory_output\"]\n",
    "                delta_batch_input_memory = new_batch_input_memory - batch_input_memory\n",
    "                delta_input_memory = delta_batch_input_memory.mean(dim=0) # (memory_size, hidden_size) # todo: mean or sum or other?\n",
    "                input_memory = input_memory + delta_input_memory * memory_lr\n",
    "\n",
    "    # last memory -> X\n",
    "    target_model_parameter = evolver_model(input_memory=input_memory.unsqueeze(0), produce_parameter_flag=True)\n",
    "    # target_model_parameter = evolver_model(input_memory=new_batch_input_memory, produce_parameter_flag=True)\n",
    "\n",
    "    # print(input_memory)\n",
    "    # print(target_model_parameter.shape)\n",
    "    \n",
    "    return target_model_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19585, 21010,    11,  4642,   287,  1446,  5250,   261,    11,  9589,\n",
      "            11,   319]], device='cuda:0')\n",
      "Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, a modest upbringing in a middle-class family. He attended a university in the Netherlands, where he attended a double-majored history class in college. After graduating from law school, he earned his law degree from University of Delaware in 1970.\n",
      "\n",
      "Biden's early career in law school was registered in the New Castle County Council in 1972. In 1972, when tragedy struck his wife and his daughter 1.88 million people killed in a tragedy, Naomi Hunter and her son were killed in a car accident, and Beau Biden, Beau and his daughter were killed in honor. Despite his commitment to honor and commitment to Biden as a senator, he chose his son as a senator from his bedside. He went from serving as Delaware's vice president for Delaware for six years, from 1973 to 2009. During his time in Washington, he was involved in various foreign and congressional committees, and was frequently involved in\n",
      "---------------\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "context = [\"\"\"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored in history and political science, graduating in 1965. Afterward, he earned his law degree from Syracuse University College of Law in 1968.\\nBiden's early political career began in 1970 when he was elected to the New Castle County Council in Delaware. In 1972, tragedy struck when his wife Neilia and 1-year-old daughter Naomi were killed in a car accident, and his two sons, Beau and Hunter, were injured. Despite this devastating loss, Biden chose to honor his commitment and was sworn in as a senator by his sons' hospital bedsides.\\nHe went on to serve as the United States Senator from Delaware for six terms, from 1973 to 2009. During his time in the Senate, Biden was involved in various committees and was particularly known for his expertise in foreign affairs, serving as the chairman of the Senate Foreign Relations Committee on multiple occasions.\\nIn 2008, Joe Biden was selected as the running mate for Barack Obama, who went on to win the presidential election. As Vice President, Biden played an integral role in the Obama administration, helping to shape policies and handling issues such as economic recovery, foreign relations, and the implementation of the Affordable Care Act (ACA), commonly known as Obamacare.\\nAfter completing two terms as Vice President, Joe Biden decided to run for the presidency in 2020. He secured the Democratic nomination and faced the incumbent President Donald Trump in the general election. Biden campaigned on a platform of unity, promising to heal the divisions in the country and tackle pressing issues, including the COVID-19 pandemic, climate change, racial justice, and economic inequality.\\nIn the November 2020 election, Biden emerged victorious, and on January 20, 2021, he was inaugurated as the 46th President of the United States. At the age of 78, Biden became the oldest person to assume the presidency in American history.\\nAs President, Joe Biden has worked to implement his agenda, focusing on various initiatives, such as infrastructure investment, climate action, immigration reform, and expanding access to healthcare. He has emphasized the importance of diplomacy in international relations and has sought to rebuild alliances with global partners.\\nThroughout his long career in public service, Joe Biden has been recognized for his commitment to bipartisanship, empathy, and his dedication to working-class issues. He continues to navigate the challenges facing the nation, striving to bring the country together and create positive change for all Americans.\"\"\"]\n",
    "input_parameter = generate_parameter(context_list=context, run_num=1)\n",
    "\n",
    "question = \"Joe Biden, born in Scranton, Pennsylvania, on\"\n",
    "generate_sentence(question)\n",
    "\n",
    "generate_sentence(question, input_parameter)\n",
    "\n",
    "question = ' The first name of the current US president is \"'\n",
    "generate_sentence(question, input_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9607, device='cuda:0')\n",
      "tensor(3.1499, device='cuda:0')\n",
      "tensor(6.8289, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context = [\"\"\"I've been reading a lot of articles on nanomedicine and crispr, and it's all really interesting to me. How much about ourselves would we be able to change once we finally crack the code of our own bodies? Would we be able to change our skeletal structure? Eye color? Could we alter our immune system to be more effective against viruses?\"\"\"]\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "\n",
    "x = \"\"\"I know currently none of this is even remotely possible, but it feels like we're dusting off the stepping stones to some pretty promising prospects, and I'm curious to see how fast things will go once we get the ball rolling. Or if, y'know, we all die before even a modicum of useful information is found.\"\"\"\n",
    "x = encode(x)\n",
    "x = torch.tensor(x, dtype=torch.long, device=device)[None, ...]\n",
    "y = x[:, 1:]\n",
    "x = x[:, :-1]\n",
    "\n",
    "encoded_context = torch.tensor(encode(context[0]), dtype=torch.long, device=device)[None, ...]\n",
    "padding_y = torch.full_like(encoded_context, fill_value=-1, dtype=torch.long, device=device)\n",
    "\n",
    "context_x = torch.concatenate((encoded_context, x), dim=1)\n",
    "context_y = torch.concatenate((padding_y, y), dim=1)\n",
    "_, loss = model(context_x, context_y)\n",
    "print(loss)\n",
    "\n",
    "_, loss = model(x, y)\n",
    "print(loss)\n",
    "\n",
    "_, loss = model(x, y, input_parameter=input_parameter)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3066539608\n",
      "tensor(3.0196, device='cuda:0')\n",
      "tensor(2.5594, device='cuda:0')\n",
      "tensor(7.7444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9256, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = \"pg19\"\n",
    "# dataset = \"openwebtext\"\n",
    "\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "# val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "print(len(train_data))\n",
    "\n",
    "####-----------------------------------------------------\n",
    "context_start = 783332\n",
    "input_start = context_start + 256\n",
    "input_end = input_start + 512\n",
    "\n",
    "# context = [decode(train_data[context_start:input_start-256]), decode(train_data[input_start-256:input_start])]\n",
    "context = [decode(train_data[context_start:input_start])]\n",
    "context_str = \"\"\n",
    "for s in context:\n",
    "    context_str += s\n",
    "\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "\n",
    "x_str = decode(train_data[input_start:input_end])\n",
    "y_str = decode(train_data[input_start + 1:input_end + 1])\n",
    "\n",
    "x = torch.from_numpy(train_data[input_start:input_end].astype(np.int64)).unsqueeze(0).to(device)\n",
    "y = torch.from_numpy(train_data[input_start + 1:input_end + 1].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "# for s in [context_str, x_str, y_str]:\n",
    "#     print(s)\n",
    "#     print(\"-\"*100)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "_, loss = model(x, y)\n",
    "print(loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "encoded_context = torch.tensor(encode(context_str), dtype=torch.long, device=device)[None, ...]\n",
    "padding_y = torch.full_like(encoded_context, fill_value=-1, dtype=torch.long, device=device)\n",
    "\n",
    "context_x = torch.concatenate((encoded_context, x), dim=1)[:, -1024:]\n",
    "context_y = torch.concatenate((padding_y, y), dim=1)[:, -1024:]\n",
    "_, loss = model(context_x, context_y)\n",
    "print(loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "_, loss = model(x, y, input_parameter=input_parameter)\n",
    "print(loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "x = torch.from_numpy(train_data[context_start:input_start].astype(np.int64)).unsqueeze(0).to(device)\n",
    "y = torch.from_numpy(train_data[context_start + 1:input_start + 1].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "_, loss = model(x, y, input_parameter=input_parameter)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erved and worried about losing her best link to a president her family takes credit for helping get elected but believes Bannon will be able to maintain his influence, people close to the family said. ….As tensions have heightened in recent weeks, the Bannon and Kushner camps have devolved into opposing firing squads. Team Bannon believes the hosts of MSNBC’s “Morning Joe,” a show the president watches regularly, are speaking regularly with Kushner and projecting his anti-Bannon sentiments. Kushner allies, meanwhile, finger Bannon as responsible for unflattering stories involving the president’s son-in-law, including those focusing on Kushner’s talks with Russians.\n",
      "\n",
      "There are factions in every White House, but Trump’s factions sound far more like a bunch of squabbling first-graders than most. “You’ve been bad-mouthing me to Joe!” “Yeah, well, who leaked that Russia stuff to the Times?”\n",
      "\n",
      "Unfortunately, the story ends with this:\n",
      "\n",
      "For Trump, one bright spot was the decision to launch 59 missiles in Syria last week. The president was pleased with the process, overseen by national security adviser H.R. McMaster, that brought together his war\n",
      "----------------------------------------------------------------------------------------------------\n",
      " cabinet and corralled its expertise in a way that resembled a more traditional White House. “He’s in the best place that I’ve seen him since the inauguration,” Barrack said. “He’s confident. He thinks he’s found the groove, and with his team too. . . . He looks great. His energy level is off the map. And I think he now feels the commander in chief role.”\n",
      "\n",
      "Great. It would be nice to have a president who felt terrible about launching missiles, even if he felt that it had to be done. But not Trump. It was the only bright spot in his week.<|endoftext|>HOBOKEN, NJ — Editor's Note: The following op-ed comes courtesy of Hoboken resident Deborah Johnson. The views expressed are not necessarily those of Patch.com.\n",
      "\n",
      "\"This country's child support laws are destroying fathers, especially young fathers, who cannot afford to honor their child support orders. These laws are breaking up minority families. These laws are contributing to the ongoing problems of domestic violence among men, women and children.\n",
      "\n",
      "\"The court needs to clarify what child support is. Is it only money? Why is being out of work, struggling to find a job, having no money, a crime in this country? A crime that threatens to send a father to jail every month for non-child support? Because the court does not agree with the amount a father pays to his child, his punishment is incarceration. All the reasons above prevent a father from paying a support order that is more money than he has ever made on a job.\n",
      "\n",
      "\"The court needs to take into consideration that when you become unemployed, you may no longer can afford to pay an assigned child support amount. If for whatever reason a father cannot support his own self and needs financial assistance, how is he going to financially support his children? Why should he be incarcerated under these conditions? How is a father ever going to fulfil his obligations if he is incarcerated? It does not constitute a crime when a father cannot buy his child an outfit one week, provide them with food throughout the day or house them in a living condition where they may have to share a bed. I consider jail to be an institute that houses people who commit crimes. The court needs to take this into consideration.\n",
      "\n",
      "\"When you take away the freedom of a father for late child support payments for one child, that man is still responsible for other children, to\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(context_str)\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(x_str)\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(41.5292) tensor(42.5889)\n",
      "tensor(3.7264) tensor(3.7516)\n"
     ]
    }
   ],
   "source": [
    "this_start = 332232\n",
    "this_length = 128\n",
    "this_segment_num = 100\n",
    "\n",
    "# rewrite the context by loop\n",
    "context = []\n",
    "for i in range(this_segment_num):\n",
    "    # context.append(decode(train_data[this_start + i * this_length:this_start + (i + 1) * this_length]))\n",
    "    context.append(train_data[this_start + i * this_length:this_start + (i + 1) * this_length])\n",
    "\n",
    "# input_parameter = generate_parameter(context_list=context, run_num=1)\n",
    "input_parameter = generate_batch_parameter(context_list=context, run_num=5, memory_lr=1.0)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "\n",
    "# gpt_loss = 0.0\n",
    "# memory_loss = 0.0\n",
    "\n",
    "# for i in range(this_segment_num):\n",
    "#     x = torch.from_numpy(train_data[this_start + i * this_length:this_start + (i + 1) * this_length].astype(np.int64)).unsqueeze(0).to(device)\n",
    "#     y = torch.from_numpy(train_data[this_start + i * this_length + 1:this_start + (i + 1) * this_length + 1].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "#     _, loss = model(x, y, input_parameter=input_parameter)\n",
    "#     print(loss)\n",
    "#     memory_loss += loss.item() / this_segment_num\n",
    "#     _, loss = model(x, y)\n",
    "#     print(loss)\n",
    "#     gpt_loss += loss.item() / this_segment_num\n",
    "#     # print(decode(train_data[this_start + i * this_length:this_start + (i + 1) * this_length + 1]))\n",
    "#     print(\"-----------------------------------------------------\")\n",
    "\n",
    "# print(memory_loss, gpt_loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "random_segment_len = 100\n",
    "\n",
    "gpt_loss = torch.tensor(0.0)\n",
    "memory_loss = torch.tensor(0.0)\n",
    "\n",
    "for _ in range(random_segment_len):\n",
    "    context_start = 34322333\n",
    "    context_start = random.randint(0, 2866539608)\n",
    "    input_start = context_start + random.randint(128, 512)\n",
    "    x = torch.from_numpy(train_data[context_start:input_start].astype(np.int64)).unsqueeze(0).to(device)\n",
    "    y = torch.from_numpy(train_data[context_start + 1:input_start + 1].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "    _, loss = model(x, y, input_parameter=input_parameter)\n",
    "    # print(loss)\n",
    "    memory_loss += loss.item() / random_segment_len\n",
    "    _, loss = model(x, y)\n",
    "    # print(loss)\n",
    "    gpt_loss += loss.item() / random_segment_len\n",
    "    # print(decode(train_data[context_start:input_start + 1]))\n",
    "    # print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(torch.exp(memory_loss), torch.exp(gpt_loss))\n",
    "print(memory_loss, gpt_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
