{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import MemoryGPT as GPT\n",
    "\n",
    "from my_configuration_roberta import MemoryRobertaConfig\n",
    "from my_modeling_roberta import MemoryRobertaModel\n",
    "\n",
    "os.environ['TIKTOKEN_CACHE_DIR']=\"/data/yuanhangyang/tiktoken_cache_dir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 2 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "\n",
    "device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "# device = 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "# exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "[27, 91, 437, 1659, 5239, 91, 29]\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = 'gpt2'\n",
    "\n",
    "# load pretrained model\n",
    "if \"gpt\" in pretrained_model:\n",
    "    print(f\"Initializing from OpenAI GPT-2 weights: {pretrained_model}\")\n",
    "    # initialize from OpenAI GPT-2 weights\n",
    "    model = GPT.from_pretrained(pretrained_model, dict(dropout=0.0))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # backbone forzen\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    \n",
    "    pretrained_model_config = model.config\n",
    "else:\n",
    "    raise Exception(f\"Unrecognized pretrained model {pretrained_model}\")\n",
    "\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# ok let's assume gpt-2 encodings by default\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "# encode = lambda s: enc.encode_ordinary(s)\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "    \n",
    "print(enc.encode_ordinary(\"<|endoftext|>\"))\n",
    "print(enc.encode(\"<|endoftext|>\",  allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load memory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 49.62M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MemoryRobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conversion_dense): Linear(in_features=768, out_features=9216, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "evolver_config = checkpoint['evolver_config']\n",
    "evolver_model = MemoryRobertaModel(evolver_config)\n",
    "state_dict = checkpoint['evolver_model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "evolver_model.load_state_dict(state_dict)\n",
    "\n",
    "evolver_model.eval()\n",
    "evolver_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(question, input_parameter=None):\n",
    "    start_ids = encode(question)\n",
    "    x = torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...]\n",
    "    print(x)\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(1):\n",
    "                # y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\n",
    "                y = model.generate(x, max_new_tokens, temperature=temperature, top_k=1, input_parameter=input_parameter)\n",
    "\n",
    "                result = y[0].tolist()[:200]\n",
    "\n",
    "                eot_index = len(result)\n",
    "                for ci, c in enumerate(result):\n",
    "                    if c == enc.eot_token:\n",
    "                        eot_index = ci\n",
    "                        break\n",
    "                \n",
    "                print(decode(result[:eot_index]))\n",
    "                print('---------------')\n",
    "            print('===============================================================')\n",
    "\n",
    "\n",
    "def generate_parameter(context_list=None, context_id_list=None):\n",
    "    if context_list is not None:\n",
    "        encoded_context = []\n",
    "\n",
    "        for c in context_list:\n",
    "            ids = encode(c)\n",
    "            # ids.append(enc.eot_token)\n",
    "            encoded_context.append(torch.tensor(ids)[None, ...].to(device))\n",
    "    \n",
    "    if context_id_list is not None:\n",
    "        encoded_context = context_id_list\n",
    "        \n",
    "    input_memory = None\n",
    "    target_model_parameter = None\n",
    "\n",
    "    for index, ec in enumerate(encoded_context):\n",
    "        output_embeds = model(idx=ec, input_parameter=target_model_parameter, output_embeds=True)\n",
    "\n",
    "        input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=input_memory)[\"memory_output\"]\n",
    "\n",
    "    # last memory -> X\n",
    "    target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "\n",
    "    # print(input_memory)\n",
    "    # print(target_model_parameter.shape)\n",
    "    \n",
    "    return target_model_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 464,  717, 1438,  286,  262, 1459, 1294, 1893,  318,  366]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mJoe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mThe first name of the current US president is \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m generate_sentence(question)\n\u001b[1;32m      5\u001b[0m context \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mJoe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, \u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m context \u001b[39m=\u001b[39m [\u001b[39m\"\"\"\u001b[39m\u001b[39mJoe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored in history and political science, graduating in 1965. Afterward, he earned his law degree from Syracuse University College of Law in 1968.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mBiden\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms early political career began in 1970 when he was elected to the New Castle County Council in Delaware. In 1972, tragedy struck when his wife Neilia and 1-year-old daughter Naomi were killed in a car accident, and his two sons, Beau and Hunter, were injured. Despite this devastating loss, Biden chose to honor his commitment and was sworn in as a senator by his sons\u001b[39m\u001b[39m'\u001b[39m\u001b[39m hospital bedsides.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mHe went on to serve as the United States Senator from Delaware for six terms, from 1973 to 2009. During his time in the Senate, Biden was involved in various committees and was particularly known for his expertise in foreign affairs, serving as the chairman of the Senate Foreign Relations Committee on multiple occasions.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIn 2008, Joe Biden was selected as the running mate for Barack Obama, who went on to win the presidential election. As Vice President, Biden played an integral role in the Obama administration, helping to shape policies and handling issues such as economic recovery, foreign relations, and the implementation of the Affordable Care Act (ACA), commonly known as Obamacare.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAfter completing two terms as Vice President, Joe Biden decided to run for the presidency in 2020. He secured the Democratic nomination and faced the incumbent President Donald Trump in the general election. Biden campaigned on a platform of unity, promising to heal the divisions in the country and tackle pressing issues, including the COVID-19 pandemic, climate change, racial justice, and economic inequality.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIn the November 2020 election, Biden emerged victorious, and on January 20, 2021, he was inaugurated as the 46th President of the United States. At the age of 78, Biden became the oldest person to assume the presidency in American history.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAs President, Joe Biden has worked to implement his agenda, focusing on various initiatives, such as infrastructure investment, climate action, immigration reform, and expanding access to healthcare. He has emphasized the importance of diplomacy in international relations and has sought to rebuild alliances with global partners.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mThroughout his long career in public service, Joe Biden has been recognized for his commitment to bipartisanship, empathy, and his dedication to working-class issues. He continues to navigate the challenges facing the nation, striving to bring the country together and create positive change for all Americans.\u001b[39m\u001b[39m\"\"\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mgenerate_sentence\u001b[0;34m(question, input_parameter)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mwith\u001b[39;00m ctx:\n\u001b[1;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m         \u001b[39m# y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m         y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(x, max_new_tokens, temperature\u001b[39m=\u001b[39;49mtemperature, top_k\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, input_parameter\u001b[39m=\u001b[39;49minput_parameter)\n\u001b[1;32m     13\u001b[0m         result \u001b[39m=\u001b[39m y[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()[:\u001b[39m200\u001b[39m]\n\u001b[1;32m     15\u001b[0m         eot_index \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(result)\n",
      "File \u001b[0;32m/data/yuanhangyang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/data/yuanhangyang/mem_nano_gpt/model.py:552\u001b[0m, in \u001b[0;36mMemoryGPT.generate\u001b[0;34m(self, idx, max_new_tokens, temperature, top_k, input_parameter, eot_token)\u001b[0m\n\u001b[1;32m    550\u001b[0m idx_cond \u001b[39m=\u001b[39m idx \u001b[39mif\u001b[39;00m idx\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mblock_size \u001b[39melse\u001b[39;00m idx[:, \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mblock_size:]\n\u001b[1;32m    551\u001b[0m \u001b[39m# forward the model to get the logits for the index in the sequence\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m logits, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(idx_cond, input_parameter\u001b[39m=\u001b[39;49minput_parameter)\n\u001b[1;32m    553\u001b[0m \u001b[39m# pluck the logits at the final step and scale by desired temperature\u001b[39;00m\n\u001b[1;32m    554\u001b[0m logits \u001b[39m=\u001b[39m logits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :] \u001b[39m/\u001b[39m temperature\n",
      "File \u001b[0;32m/data/yuanhangyang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/yuanhangyang/mem_nano_gpt/model.py:417\u001b[0m, in \u001b[0;36mMemoryGPT.forward\u001b[0;34m(self, idx, targets, input_parameter, output_embeds)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    415\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcheck here (input memory dimension) !\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 417\u001b[0m     x \u001b[39m=\u001b[39m block(x, input_parameter\u001b[39m=\u001b[39;49mthis_input_parameter)\n\u001b[1;32m    418\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mln_f(x)\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[39m# if we are given some desired targets also calculate the loss\u001b[39;00m\n",
      "File \u001b[0;32m/data/yuanhangyang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/yuanhangyang/mem_nano_gpt/model.py:111\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, input_parameter)\u001b[0m\n\u001b[1;32m    108\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(x))\n\u001b[1;32m    109\u001b[0m     next_x \u001b[39m=\u001b[39m x[:, input_parameter\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:, :]\n\u001b[0;32m--> 111\u001b[0m next_x \u001b[39m=\u001b[39m next_x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_2(next_x))\n\u001b[1;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m next_x\n",
      "File \u001b[0;32m/data/yuanhangyang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/yuanhangyang/mem_nano_gpt/model.py:88\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 88\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_fc(x)\n\u001b[1;32m     89\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(x)\n\u001b[1;32m     90\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(x)\n",
      "File \u001b[0;32m/data/yuanhangyang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data/yuanhangyang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "question = \"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored\"\n",
    "question = 'The first name of the current US president is \"'\n",
    "generate_sentence(question)\n",
    "\n",
    "context = [\"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, \"]\n",
    "context = [\"\"\"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored in history and political science, graduating in 1965. Afterward, he earned his law degree from Syracuse University College of Law in 1968.\\nBiden's early political career began in 1970 when he was elected to the New Castle County Council in Delaware. In 1972, tragedy struck when his wife Neilia and 1-year-old daughter Naomi were killed in a car accident, and his two sons, Beau and Hunter, were injured. Despite this devastating loss, Biden chose to honor his commitment and was sworn in as a senator by his sons' hospital bedsides.\\nHe went on to serve as the United States Senator from Delaware for six terms, from 1973 to 2009. During his time in the Senate, Biden was involved in various committees and was particularly known for his expertise in foreign affairs, serving as the chairman of the Senate Foreign Relations Committee on multiple occasions.\\nIn 2008, Joe Biden was selected as the running mate for Barack Obama, who went on to win the presidential election. As Vice President, Biden played an integral role in the Obama administration, helping to shape policies and handling issues such as economic recovery, foreign relations, and the implementation of the Affordable Care Act (ACA), commonly known as Obamacare.\\nAfter completing two terms as Vice President, Joe Biden decided to run for the presidency in 2020. He secured the Democratic nomination and faced the incumbent President Donald Trump in the general election. Biden campaigned on a platform of unity, promising to heal the divisions in the country and tackle pressing issues, including the COVID-19 pandemic, climate change, racial justice, and economic inequality.\\nIn the November 2020 election, Biden emerged victorious, and on January 20, 2021, he was inaugurated as the 46th President of the United States. At the age of 78, Biden became the oldest person to assume the presidency in American history.\\nAs President, Joe Biden has worked to implement his agenda, focusing on various initiatives, such as infrastructure investment, climate action, immigration reform, and expanding access to healthcare. He has emphasized the importance of diplomacy in international relations and has sought to rebuild alliances with global partners.\\nThroughout his long career in public service, Joe Biden has been recognized for his commitment to bipartisanship, empathy, and his dedication to working-class issues. He continues to navigate the challenges facing the nation, striving to bring the country together and create positive change for all Americans.\"\"\"]\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "question = \"where he double-majored\"\n",
    "question = 'The first name of the current US president is \"'\n",
    "generate_sentence(question, input_parameter)\n",
    "\n",
    "generate_sentence(question)\n",
    "\n",
    "\n",
    "# context = [\"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\", \"i am a handsome chinese boy. I live in Beijing now.\"]\n",
    "# context = [\"We saw that Reinforce worked well. However, because we use Monte-Carlo sampling to estimate return (we use an entire episode to calculate the return), we have significant variance in policy gradient estimation.\", \n",
    "#            \"i am a handsome chinese boy. I love Japanese anime girls.\"\n",
    "#            \"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.\",\n",
    "#            ]\n",
    "# context = [\"i am a handsome chinese boy.\"]\n",
    "context = [\"Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.\"]\n",
    "context = [\"\"\"Joe Biden, born in Scranton, Pennsylvania, on November 20, 1942, had a modest upbringing in a middle-class family. He attended the University of Delaware, where he double-majored in history and political science, graduating in 1965. Afterward, he earned his law degree from Syracuse University College of Law in 1968.\\nBiden's early political career began in 1970 when he was elected to the New Castle County Council in Delaware. In 1972, tragedy struck when his wife Neilia and 1-year-old daughter Naomi were killed in a car accident, and his two sons, Beau and Hunter, were injured. Despite this devastating loss, Biden chose to honor his commitment and was sworn in as a senator by his sons' hospital bedsides.\\nHe went on to serve as the United States Senator from Delaware for six terms, from 1973 to 2009. During his time in the Senate, Biden was involved in various committees and was particularly known for his expertise in foreign affairs, serving as the chairman of the Senate Foreign Relations Committee on multiple occasions.\\nIn 2008, Joe Biden was selected as the running mate for Barack Obama, who went on to win the presidential election. As Vice President, Biden played an integral role in the Obama administration, helping to shape policies and handling issues such as economic recovery, foreign relations, and the implementation of the Affordable Care Act (ACA), commonly known as Obamacare.\\nAfter completing two terms as Vice President, Joe Biden decided to run for the presidency in 2020. He secured the Democratic nomination and faced the incumbent President Donald Trump in the general election. Biden campaigned on a platform of unity, promising to heal the divisions in the country and tackle pressing issues, including the COVID-19 pandemic, climate change, racial justice, and economic inequality.\\nIn the November 2020 election, Biden emerged victorious, and on January 20, 2021, he was inaugurated as the 46th President of the United States. At the age of 78, Biden became the oldest person to assume the presidency in American history.\\nAs President, Joe Biden has worked to implement his agenda, focusing on various initiatives, such as infrastructure investment, climate action, immigration reform, and expanding access to healthcare. He has emphasized the importance of diplomacy in international relations and has sought to rebuild alliances with global partners.\\nThroughout his long career in public service, Joe Biden has been recognized for his commitment to bipartisanship, empathy, and his dedication to working-class issues. He continues to navigate the challenges facing the nation, striving to bring the country together and create positive change for all Americans.\"\"\"]\n",
    "context = ['The name of the current US president is \"Joe Biden\".']\n",
    "context = ['Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. ']\n",
    "\n",
    "question = \"However, conventional approaches fine-tune all the \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9607)\n",
      "tensor(3.1497)\n",
      "tensor(6.7931, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context = [\"\"\"I've been reading a lot of articles on nanomedicine and crispr, and it's all really interesting to me. How much about ourselves would we be able to change once we finally crack the code of our own bodies? Would we be able to change our skeletal structure? Eye color? Could we alter our immune system to be more effective against viruses?\"\"\"]\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "\n",
    "x = \"\"\"I know currently none of this is even remotely possible, but it feels like we're dusting off the stepping stones to some pretty promising prospects, and I'm curious to see how fast things will go once we get the ball rolling. Or if, y'know, we all die before even a modicum of useful information is found.\"\"\"\n",
    "x = encode(x)\n",
    "x = torch.tensor(x, dtype=torch.long, device=device)[None, ...]\n",
    "y = x[:, 1:]\n",
    "x = x[:, :-1]\n",
    "\n",
    "encoded_context = torch.tensor(encode(context[0]), dtype=torch.long, device=device)[None, ...]\n",
    "padding_y = torch.full_like(encoded_context, fill_value=-1, dtype=torch.long, device=device)\n",
    "\n",
    "context_x = torch.concatenate((encoded_context, x), dim=1)\n",
    "context_y = torch.concatenate((padding_y, y), dim=1)\n",
    "_, loss = model(context_x, context_y)\n",
    "print(loss)\n",
    "\n",
    "_, loss = model(x, y)\n",
    "print(loss)\n",
    "\n",
    "_, loss = model(x, y, input_parameter=input_parameter)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9035582489\n",
      "tensor(3.1177)\n",
      "tensor(2.9636)\n",
      "tensor(6.3444, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5791, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dataset = \"pg19\"\n",
    "dataset = \"openwebtext\"\n",
    "\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "# val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "print(len(train_data))\n",
    "\n",
    "####-----------------------------------------------------\n",
    "context_start = 783332\n",
    "input_start = context_start + 256\n",
    "input_end = input_start + 512\n",
    "\n",
    "# context = [decode(train_data[context_start:input_start-256]), decode(train_data[input_start-256:input_start])]\n",
    "context = [decode(train_data[context_start:input_start])]\n",
    "context_str = \"\"\n",
    "for s in context:\n",
    "    context_str += s\n",
    "\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "\n",
    "x_str = decode(train_data[input_start:input_end])\n",
    "y_str = decode(train_data[input_start + 1:input_end + 1])\n",
    "\n",
    "x = torch.from_numpy(train_data[input_start:input_end].astype(np.int64)).unsqueeze(0).to(device)\n",
    "y = torch.from_numpy(train_data[input_start + 1:input_end + 1].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "# for s in [context_str, x_str, y_str]:\n",
    "#     print(s)\n",
    "#     print(\"-\"*100)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "_, loss = model(x, y)\n",
    "print(loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "encoded_context = torch.tensor(encode(context_str), dtype=torch.long, device=device)[None, ...]\n",
    "padding_y = torch.full_like(encoded_context, fill_value=-1, dtype=torch.long, device=device)\n",
    "\n",
    "context_x = torch.concatenate((encoded_context, x), dim=1)[:, -1024:]\n",
    "context_y = torch.concatenate((padding_y, y), dim=1)[:, -1024:]\n",
    "_, loss = model(context_x, context_y)\n",
    "print(loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "_, loss = model(x, y, input_parameter=input_parameter)\n",
    "print(loss)\n",
    "\n",
    "####-----------------------------------------------------\n",
    "x = torch.from_numpy(train_data[context_start:input_start].astype(np.int64)).unsqueeze(0).to(device)\n",
    "y = torch.from_numpy(train_data[context_start + 1:input_start + 1].astype(np.int64)).unsqueeze(0).to(device)\n",
    "\n",
    "_, loss = model(x, y, input_parameter=input_parameter)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erved and worried about losing her best link to a president her family takes credit for helping get elected but believes Bannon will be able to maintain his influence, people close to the family said. ….As tensions have heightened in recent weeks, the Bannon and Kushner camps have devolved into opposing firing squads. Team Bannon believes the hosts of MSNBC’s “Morning Joe,” a show the president watches regularly, are speaking regularly with Kushner and projecting his anti-Bannon sentiments. Kushner allies, meanwhile, finger Bannon as responsible for unflattering stories involving the president’s son-in-law, including those focusing on Kushner’s talks with Russians.\n",
      "\n",
      "There are factions in every White House, but Trump’s factions sound far more like a bunch of squabbling first-graders than most. “You’ve been bad-mouthing me to Joe!” “Yeah, well, who leaked that Russia stuff to the Times?”\n",
      "\n",
      "Unfortunately, the story ends with this:\n",
      "\n",
      "For Trump, one bright spot was the decision to launch 59 missiles in Syria last week. The president was pleased with the process, overseen by national security adviser H.R. McMaster, that brought together his war\n",
      "----------------------------------------------------------------------------------------------------\n",
      " cabinet and corralled its expertise in a way that resembled a more traditional White House. “He’s in the best place that I’ve seen him since the inauguration,” Barrack said. “He’s confident. He thinks he’s found the groove, and with his team too. . . . He looks great. His energy level is off the map. And I think he now feels the commander in chief role.”\n",
      "\n",
      "Great. It would be nice to have a president who felt terrible about launching missiles, even if he felt that it had to be done. But not Trump. It was the only bright spot in his week.<|endoftext|>HOBOKEN, NJ — Editor's Note: The following op-ed comes courtesy of Hoboken resident Deborah Johnson. The views expressed are not necessarily those of Patch.com.\n",
      "\n",
      "\"This country's child support laws are destroying fathers, especially young fathers, who cannot afford to honor their child support orders. These laws are breaking up minority families. These laws are contributing to the ongoing problems of domestic violence among men, women and children.\n",
      "\n",
      "\"The court needs to clarify what child support is. Is it only money? Why is being out of work, struggling to find a job, having no money, a crime in this country? A crime that threatens to send a father to jail every month for non-child support? Because the court does not agree with the amount a father pays to his child, his punishment is incarceration. All the reasons above prevent a father from paying a support order that is more money than he has ever made on a job.\n",
      "\n",
      "\"The court needs to take into consideration that when you become unemployed, you may no longer can afford to pay an assigned child support amount. If for whatever reason a father cannot support his own self and needs financial assistance, how is he going to financially support his children? Why should he be incarcerated under these conditions? How is a father ever going to fulfil his obligations if he is incarcerated? It does not constitute a crime when a father cannot buy his child an outfit one week, provide them with food throughout the day or house them in a living condition where they may have to share a bed. I consider jail to be an institute that houses people who commit crimes. The court needs to take this into consideration.\n",
      "\n",
      "\"When you take away the freedom of a father for late child support payments for one child, that man is still responsible for other children, to\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(context_str)\n",
    "print(\"-\"*100)\n",
    "\n",
    "print(x_str)\n",
    "print(\"-\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
