{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from my_modeling_opt import OPTForCausalLM\n",
    "import torch\n",
    "from my_utils import get_seq_train_batch\n",
    "import numpy as np\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'bfloat16' if torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m OPTForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/opt-2.7b\u001b[39;49m\u001b[39m\"\u001b[39;49m, load_in_8bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device_map\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m: device}, torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16, cache_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/data/yuanhang/hf_cache\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m model \u001b[39m=\u001b[39m prepare_model_for_kbit_training(model)\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/transformers/modeling_utils.py:2828\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[39m# Dispatch model with hooks on all devices if necessary\u001b[39;00m\n\u001b[1;32m   2827\u001b[0m \u001b[39mif\u001b[39;00m device_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2828\u001b[0m     dispatch_model(model, device_map\u001b[39m=\u001b[39;49mdevice_map, offload_dir\u001b[39m=\u001b[39;49moffload_folder, offload_index\u001b[39m=\u001b[39;49moffload_index)\n\u001b[1;32m   2830\u001b[0m \u001b[39mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   2831\u001b[0m     \u001b[39mif\u001b[39;00m loading_info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/big_modeling.py:371\u001b[0m, in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes)\u001b[0m\n\u001b[1;32m    368\u001b[0m     weights_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    370\u001b[0m tied_params \u001b[39m=\u001b[39m find_tied_parameters(model)\n\u001b[0;32m--> 371\u001b[0m attach_align_device_hook_on_blocks(\n\u001b[1;32m    372\u001b[0m     model,\n\u001b[1;32m    373\u001b[0m     execution_device\u001b[39m=\u001b[39;49mexecution_device,\n\u001b[1;32m    374\u001b[0m     offload\u001b[39m=\u001b[39;49moffload,\n\u001b[1;32m    375\u001b[0m     offload_buffers\u001b[39m=\u001b[39;49moffload_buffers,\n\u001b[1;32m    376\u001b[0m     weights_map\u001b[39m=\u001b[39;49mweights_map,\n\u001b[1;32m    377\u001b[0m     skip_keys\u001b[39m=\u001b[39;49mskip_keys,\n\u001b[1;32m    378\u001b[0m     preload_module_classes\u001b[39m=\u001b[39;49mpreload_module_classes,\n\u001b[1;32m    379\u001b[0m )\n\u001b[1;32m    380\u001b[0m \u001b[39m# Attaching the hook may break tied weights, so we retie them\u001b[39;00m\n\u001b[1;32m    381\u001b[0m retie_parameters(model, tied_params)\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/hooks.py:506\u001b[0m, in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39min\u001b[39;00m execution_device \u001b[39mand\u001b[39;00m module_name \u001b[39min\u001b[39;00m offload \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m offload[module_name]:\n\u001b[1;32m    499\u001b[0m     hook \u001b[39m=\u001b[39m AlignDevicesHook(\n\u001b[1;32m    500\u001b[0m         execution_device\u001b[39m=\u001b[39mexecution_device[module_name],\n\u001b[1;32m    501\u001b[0m         offload_buffers\u001b[39m=\u001b[39moffload_buffers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m         skip_keys\u001b[39m=\u001b[39mskip_keys,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[0;32m--> 506\u001b[0m     add_hook_to_module(module, hook)\n\u001b[1;32m    507\u001b[0m     attach_execution_device_hook(module, execution_device[module_name])\n\u001b[1;32m    508\u001b[0m \u001b[39melif\u001b[39;00m module_name \u001b[39min\u001b[39;00m execution_device \u001b[39mand\u001b[39;00m module_name \u001b[39min\u001b[39;00m offload:\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/hooks.py:155\u001b[0m, in \u001b[0;36madd_hook_to_module\u001b[0;34m(module, hook, append)\u001b[0m\n\u001b[1;32m    152\u001b[0m     old_forward \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39mforward\n\u001b[1;32m    153\u001b[0m     module\u001b[39m.\u001b[39m_old_forward \u001b[39m=\u001b[39m old_forward\n\u001b[0;32m--> 155\u001b[0m module \u001b[39m=\u001b[39m hook\u001b[39m.\u001b[39;49minit_hook(module)\n\u001b[1;32m    156\u001b[0m module\u001b[39m.\u001b[39m_hf_hook \u001b[39m=\u001b[39m hook\n\u001b[1;32m    158\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(old_forward)\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_forward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/hooks.py:253\u001b[0m, in \u001b[0;36mAlignDevicesHook.init_hook\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffload \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecution_device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m     \u001b[39mfor\u001b[39;00m name, _ \u001b[39min\u001b[39;00m named_module_tensors(module, recurse\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_submodules):\n\u001b[0;32m--> 253\u001b[0m         set_module_tensor_to_device(module, name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecution_device)\n\u001b[1;32m    254\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffload:\n\u001b[1;32m    255\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_devices \u001b[39m=\u001b[39m {\n\u001b[1;32m    256\u001b[0m         name: param\u001b[39m.\u001b[39mdevice \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m named_module_tensors(module, recurse\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_submodules)\n\u001b[1;32m    257\u001b[0m     }\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/accelerate/utils/modeling.py:346\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics)\u001b[0m\n\u001b[1;32m    344\u001b[0m                 module\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mcuda(device_index)\n\u001b[1;32m    345\u001b[0m \u001b[39m# clean pre and post foward hook\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mempty_cache()\n",
      "File \u001b[0;32m/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/torch/cuda/memory.py:133\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Releases all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 133\u001b[0m     torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_cuda_emptyCache()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = OPTForCausalLM.from_pretrained(\"facebook/opt-2.7b\", load_in_8bit=True, device_map={'': device}, torch_dtype=torch.float16, cache_dir=\"/data/yuanhang/hf_cache\")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, p in model.named_parameters():\n",
    "#     print(n, p.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "tokenizer.pad_token = model.model.padding_idx\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,  343,  574, 8071]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"yangyy\", return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(question, input_parameter=None, peft=\"prompt\"):\n",
    "    x = tokenizer(question, return_tensors=\"pt\", padding=True)\n",
    "    x.to(device)\n",
    "    print(x.input_ids.shape)\n",
    "    # print(x)\n",
    "\n",
    "    # run generation\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            for k in range(1):\n",
    "                # y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k, input_parameter=target_model_parameter)\n",
    "                y = model.generate(x.input_ids, max_length=x.input_ids.shape[1] + 128, input_parameter=input_parameter, peft=peft)\n",
    "\n",
    "                y = y[:, x.input_ids.shape[1]:]\n",
    "\n",
    "                result = tokenizer.batch_decode(y, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "                \n",
    "                print(result[0])\n",
    "                print('---------------')\n",
    "            print('===============================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 264])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B \n",
      "Question: Kant’s moral theory is based on the idea that: \n",
      " A: the human will is the source of all moral value. B: humans are inherently good. C: humans are inherently evil. D: humans are not inherently good or evil. \n",
      " Answer: A \n",
      "Question: According to Kant, if a moral rule is a good one, then it must be a _____. \n",
      " A: rational one B: universal one C: moral one D: empirical one. \n",
      " Answer: B \n",
      "Question: Kant’s moral theory is based on the idea that: \n",
      " A: the human will is the source of all moral value. B: humans are inherently good. C: humans are inherently evil. D: humans are not inherently good or evil. \n",
      " Answer: A \n",
      "Question: According to Kant, if a moral rule is a good one, then it must be a _____. \n",
      " A: rational one B: universal one C: moral one D: empirical one. \n",
      " Answer: B \n",
      "Question: According to Kant, the categorical imperative is a moral rule that is _____\n",
      "---------------\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "question = [\n",
    "    'Question: Aesthetics deals with objects that are_____. \\n A: essential to our existence B: unimportant to most people C: not essential to our existence D: rarely viewed. \\n Answer: C \\n',\n",
    "    'Question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____. \\n A: the state B: the justice system C: the body D: the soul. \\n Answer: D \\n',\n",
    "    'Question: For Socrates, the soul is harmed by lack of _____. \\n A: knowledge B: wealth C: community D: courage. \\n Answer: A \\n',\n",
    "    'Question: According to Kant, nothing can be called “good” without qualification except _____. \\n A: right action B: good consequences C: happiness D: a good will. \\n Answer: D \\n',\n",
    "    'Question: Baier argues that genuine moral rules: \\n A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \\n Answer:',\n",
    "    # \"Question: Plato's view is that true beauty is _____. \\n A: found in everyday objects B: nonexistent C: everywhere in the natural world D: not of this world. \\n \",\n",
    "    ]\n",
    "\n",
    "question_str = \"\"\n",
    "for q in question:\n",
    "    question_str += q\n",
    "\n",
    "generate_sentence([question_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yuanhang/anaconda3/envs/moe/lib/python3.8/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2665, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_data = np.memmap(\"./data/llama_openwebtext/train.bin\", dtype=np.uint16, mode='r')\n",
    "\n",
    "for _ in range(20):\n",
    "    with torch.no_grad():\n",
    "        data_pointer, x, y, attention_mask, seg_length_list = get_seq_train_batch(train_data, [random.randint(0, 100000000)], 16, 256, 128, device, device_type, False)\n",
    "\n",
    "        x = x.squeeze(0)\n",
    "        y = y.squeeze(0)\n",
    "        attention_mask = attention_mask.squeeze(0)\n",
    "\n",
    "        # print(x[0])\n",
    "        # print(y[0])\n",
    "\n",
    "        output = model(x, attention_mask=attention_mask, labels=y)\n",
    "        print(output.loss)\n",
    "\n",
    "        break\n",
    "\n",
    "        # output = other_model(x, attention_mask=attention_mask, labels=y)\n",
    "        # print(output.loss)\n",
    "        \n",
    "        # if torch.isnan(output.loss):\n",
    "        #     print(\"nan!!!\")\n",
    "        # print(\"---\"*20)\n",
    "\n",
    "        # out = model(x, output_embeds=True)\n",
    "        # print(out.shape, x)\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4192, device='cuda:1')\n",
      "tensor(1.4192, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shift_logits = output.logits.view(-1, model.config.vocab_size)\n",
    "shift_labels = y.view(-1)\n",
    "\n",
    "loss_fct = CrossEntropyLoss(ignore_index=-1, reduction='mean')\n",
    "loss = loss_fct(shift_logits, shift_labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load memory model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1342.66M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MemoryRobertaModel(\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (key): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (value): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "          (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conversion_dense): Linear(in_features=4096, out_features=131072, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_modeling_roberta import MemoryRobertaModel\n",
    "\n",
    "out_dir = 'llama_out'\n",
    "\n",
    "peft_method = \"prompt\"\n",
    "# peft_method = \"lora\"\n",
    "\n",
    "ckpt_path = os.path.join(out_dir, 'latest_predict_1st_1seg_m20_llamaa.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "evolver_config = checkpoint['evolver_config']\n",
    "evolver_model = MemoryRobertaModel(evolver_config)\n",
    "state_dict = checkpoint['evolver_model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "evolver_model.load_state_dict(state_dict)\n",
    "\n",
    "evolver_model.eval()\n",
    "evolver_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint[\"iter_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parameter(context_list=None, context_id_list=None):\n",
    "    if context_list is not None:\n",
    "        encoded_context = []\n",
    "\n",
    "        for c in context_list:\n",
    "            ids = tokenizer(c, return_tensors=\"pt\", padding=True)\n",
    "            # ids.append(enc.eot_token)\n",
    "            encoded_context.append(ids.input_ids.to(device))\n",
    "    \n",
    "    if context_id_list is not None:\n",
    "        encoded_context = context_id_list\n",
    "        \n",
    "    input_memory = None\n",
    "    target_model_parameter = None\n",
    "\n",
    "    for index, ec in enumerate(encoded_context):\n",
    "        output_embeds = model(input_ids=ec, output_embeds=True, return_dict=False)\n",
    "\n",
    "        input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=input_memory)[\"memory_output\"]\n",
    "\n",
    "    # last memory -> X\n",
    "    target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "\n",
    "    # print(input_memory)\n",
    "    # print(target_model_parameter.shape)\n",
    "    \n",
    "    return target_model_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 73])\n",
      "B\n",
      "Explanation: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Question: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Answer: B\n",
      "Explanation: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Question: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Answer: B\n",
      "Explanation: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Question: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Answer: B\n",
      "Explanation: Ba\n",
      "---------------\n",
      "===============================================================\n",
      "torch.Size([1, 73])\n",
      "A\n",
      "Explanation: \n",
      "Baier argues that genuine moral rules must be for the good of human beings.\n",
      "Question: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Answer: A\n",
      "Explanation: \n",
      "Baier argues that genuine moral rules must be for the good of human beings.\n",
      "Question: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Answer: A\n",
      "Explanation: \n",
      "Baier argues that genuine moral rules must be for the good of human beings.\n",
      "Question: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Answer: A\n",
      "Explanation: \n",
      "Baier argues that genuine moral rules must be for the good of human beings.\n",
      "Question: Baier argues that genuine moral rules:  A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \n",
      "Answer: A\n",
      "Explanation: \n",
      "Baier argues that genuine moral rules must be for the good of human beings.\n",
      "Question: Baier argues that genuine moral\n",
      "---------------\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "context = [\n",
    "    'Question: Aesthetics deals with objects that are_____. \\n A: essential to our existence B: unimportant to most people C: not essential to our existence D: rarely viewed. \\n Answer: C \\n',\n",
    "    'Question: For Socrates, an unexamined life is a tragedy because it results in grievous harm to _____. \\n A: the state B: the justice system C: the body D: the soul. \\n Answer: D \\n',\n",
    "    'Question: For Socrates, the soul is harmed by lack of _____. \\n A: knowledge B: wealth C: community D: courage. \\n Answer: A \\n',\n",
    "    'Question: According to Kant, nothing can be called “good” without qualification except _____. \\n A: right action B: good consequences C: happiness D: a good will. \\n Answer: D \\n',\n",
    "    # \"Question: Plato's view is that true beauty is _____. \\n A: found in everyday objects B: nonexistent C: everywhere in the natural world D: not of this world. \\n \",\n",
    "    ]\n",
    "\n",
    "context_str = \"\"\n",
    "for c in context:\n",
    "    context_str += q\n",
    "\n",
    "question = 'Question: Baier argues that genuine moral rules: \\n A: must be for the good of human beings. B: make take into account the interests of all sentient beings. C: must take into account the interests of all living beings. D: are primarily directed toward promoting self-interest. \\n Answer:',\n",
    "\n",
    "input_parameter = generate_parameter(context_list=[context])\n",
    "\n",
    "generate_sentence(question, input_parameter, peft_method)\n",
    "generate_sentence(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not \"Barak\").\n",
      "\n",
      "The first name of the current US president is \"Barack\" (not \"Barak\"), and the first name of the president of the United States is \"Barack\" (not\n",
      "---------------\n",
      "===============================================================\n",
      "torch.Size([1, 12])\n",
      "George\" (not \"George W.\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"W\" (not \"W. Bush\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of the current US president is \"Bush\" (not \"Bush II\").\n",
      "\n",
      "  The second name of\n",
      "---------------\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "context = [\"Joe Biden is the current president of the United States of America.\"]\n",
    "question = ' The first name of the current US president is \"'\n",
    "\n",
    "input_parameter = generate_parameter(context_list=context)\n",
    "\n",
    "generate_sentence(question, input_parameter, peft_method)\n",
    "generate_sentence(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7204, device='cuda:0')\n",
      "tensor(2.5149, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.4255, device='cuda:0')\n",
      "tensor(2.3464, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(1.8927, device='cuda:0')\n",
      "tensor(1.8095, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.7253, device='cuda:0')\n",
      "tensor(2.5665, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.1516, device='cuda:0')\n",
      "tensor(1.9360, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.7536, device='cuda:0')\n",
      "tensor(2.5290, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.3158, device='cuda:0')\n",
      "tensor(2.1878, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.4065, device='cuda:0')\n",
      "tensor(2.1413, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.2744, device='cuda:0')\n",
      "tensor(2.1364, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(1.8700, device='cuda:0')\n",
      "tensor(1.7909, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.2895, device='cuda:0')\n",
      "tensor(2.1948, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.0684, device='cuda:0')\n",
      "tensor(1.8518, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(1.5323, device='cuda:0')\n",
      "tensor(1.5203, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(3.2276, device='cuda:0')\n",
      "tensor(2.8851, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.6920, device='cuda:0')\n",
      "tensor(2.5861, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(1.6709, device='cuda:0')\n",
      "tensor(1.5670, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.3100, device='cuda:0')\n",
      "tensor(2.2560, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.3152, device='cuda:0')\n",
      "tensor(2.2408, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(2.3079, device='cuda:0')\n",
      "tensor(2.1937, device='cuda:0')\n",
      "------------------------------------------------------------\n",
      "tensor(3.1157, device='cuda:0')\n",
      "tensor(3.1007, device='cuda:0')\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "train_data = np.memmap(\"./data/llama_openwebtext/train.bin\", dtype=np.uint16, mode='r')\n",
    "\n",
    "for _ in range(20):\n",
    "    with torch.no_grad():\n",
    "        data_pointer, x, y, attention_mask, seg_length_list = get_seq_train_batch(train_data, [random.randint(0, 100000000)], 16, 256, 128, device, device_type, False)\n",
    "\n",
    "        context = x.squeeze(0)[0:1]\n",
    "\n",
    "        x = x.squeeze(0)[1:2]\n",
    "        y = y.squeeze(0)[1:2]\n",
    "        attention_mask = attention_mask.squeeze(0)[1:2]\n",
    "\n",
    "        output_embeds = model(input_ids=context, output_embeds=True, return_dict=False)\n",
    "        input_memory = evolver_model(inputs_embeds=output_embeds, input_memory=None)[\"memory_output\"]\n",
    "        target_model_parameter = evolver_model(input_memory=input_memory, produce_parameter_flag=True)\n",
    "\n",
    "        # print(x[0])\n",
    "        # print(y[0])\n",
    "\n",
    "        output = model(x, attention_mask=attention_mask, labels=y)\n",
    "        print(output.loss)\n",
    "\n",
    "        output = model(x, attention_mask=attention_mask, labels=y, input_parameter=input_parameter, peft=\"prompt\")\n",
    "        print(output.loss)\n",
    "\n",
    "        # output = other_model(x, attention_mask=attention_mask, labels=y)\n",
    "        # print(output.loss)\n",
    "        \n",
    "        # if torch.isnan(output.loss):\n",
    "        #     print(\"nan!!!\")\n",
    "        print(\"---\"*20)\n",
    "\n",
    "        # out = model(x, output_embeds=True)\n",
    "        # print(out.shape, x)\n",
    "        # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
